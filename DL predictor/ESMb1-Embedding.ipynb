{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:41.464071Z",
     "iopub.status.busy": "2025-03-25T10:50:41.463782Z",
     "iopub.status.idle": "2025-03-25T10:50:44.818474Z",
     "shell.execute_reply": "2025-03-25T10:50:44.817234Z",
     "shell.execute_reply.started": "2025-03-25T10:50:41.464051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q fair-esm peft transformers torch biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:44.820487Z",
     "iopub.status.busy": "2025-03-25T10:50:44.820123Z",
     "iopub.status.idle": "2025-03-25T10:50:44.826555Z",
     "shell.execute_reply": "2025-03-25T10:50:44.825728Z",
     "shell.execute_reply.started": "2025-03-25T10:50:44.820453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "Data_dir = \"/kaggle/input/south-north-gasaid\"\n",
    "filenames = [\"south-animal-H1N1.fasta\",\"south-animal-H3N2.fasta\",\"south-human-H1N1.fasta\" ,\"south-human-H3N2.fasta\"]\n",
    "file_paths = [os.path.join(Data_dir, file) for file in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:44.828389Z",
     "iopub.status.busy": "2025-03-25T10:50:44.828164Z",
     "iopub.status.idle": "2025-03-25T10:50:44.847852Z",
     "shell.execute_reply": "2025-03-25T10:50:44.847063Z",
     "shell.execute_reply.started": "2025-03-25T10:50:44.828370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def readDataFromFile(filenames):\n",
    "    dfs = []  \n",
    "\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.abspath(filename)  \n",
    "        df = pd.DataFrame.from_records([\n",
    "            {\n",
    "                \"Class\": (record.description.split(\"|\")[-2]) if \"|\" in record.description else record.description,  \n",
    "                \"virus_ID\": \"|\".join(record.description.split(\"|\")[2:]) if \"|\" in record.description else record.description,\n",
    "                \"seq_ID\": \"|\".join(record.description.split(\"|\")[1:2]) if \"|\" in record.description else record.description,\n",
    "                \"Sequence\": str(record.seq),  \n",
    "                \"Length\": len(record.seq),  \n",
    "            }\n",
    "            for record in SeqIO.parse(file_path, \"fasta\")\n",
    "        ])\n",
    "          \n",
    "        dfs.append(df)  \n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:44.849118Z",
     "iopub.status.busy": "2025-03-25T10:50:44.848863Z",
     "iopub.status.idle": "2025-03-25T10:50:44.892703Z",
     "shell.execute_reply": "2025-03-25T10:50:44.892040Z",
     "shell.execute_reply.started": "2025-03-25T10:50:44.849086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/south-north-gasaid/south-animal-H1N1.fasta', '/kaggle/input/south-north-gasaid/south-animal-H3N2.fasta']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1637 entries, 0 to 1636\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Class     1637 non-null   object\n",
      " 1   virus_ID  1637 non-null   object\n",
      " 2   seq_ID    1637 non-null   object\n",
      " 3   Sequence  1637 non-null   object\n",
      " 4   Length    1637 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 64.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(file_paths[0:2])\n",
    "df=readDataFromFile(file_paths[0:2])\n",
    "\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:44.919378Z",
     "iopub.status.busy": "2025-03-25T10:50:44.919126Z",
     "iopub.status.idle": "2025-03-25T10:50:46.425432Z",
     "shell.execute_reply": "2025-03-25T10:50:46.424555Z",
     "shell.execute_reply.started": "2025-03-25T10:50:44.919360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-32): 33 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ESM-2 Model & Tokenizer\n",
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  # Fast tokenizer\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()  # Set model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:46.427667Z",
     "iopub.status.busy": "2025-03-25T10:50:46.427382Z",
     "iopub.status.idle": "2025-03-25T10:50:46.433091Z",
     "shell.execute_reply": "2025-03-25T10:50:46.432282Z",
     "shell.execute_reply.started": "2025-03-25T10:50:46.427645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return row[\"Sequence\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences = batch  \n",
    "\n",
    "    tokenized_batch = tokenizer(list(sequences), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return tokenized_batch.to(device)  \n",
    "\n",
    "dataset = SequenceDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=20, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:50:46.434367Z",
     "iopub.status.busy": "2025-03-25T10:50:46.434096Z",
     "iopub.status.idle": "2025-03-25T10:56:15.979809Z",
     "shell.execute_reply": "2025-03-25T10:56:15.979109Z",
     "shell.execute_reply.started": "2025-03-25T10:50:46.434346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference:   0%|          | 0/82 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Running Inference: 100%|██████████| 82/82 [05:29<00:00,  4.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1637, 1280])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(df)\n",
    "hidden_dim = model.config.hidden_size  # Get embedding dimension from model\n",
    "cls_embeddings = torch.zeros((num_samples, hidden_dim), device=device)  \n",
    "index = 0  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "        tokenized_inputs = batch  \n",
    "        results = model(**tokenized_inputs, output_hidden_states=True)\n",
    "\n",
    "        # Extract CLS Embeddings (First token from last layer)\n",
    "        batch_cls_embeddings = results.hidden_states[-1][:, 0, :]  # Shape: (batch_size, hidden_dim)\n",
    "        batch_size = batch_cls_embeddings.shape[0]\n",
    "        cls_embeddings[index : index + batch_size] = batch_cls_embeddings\n",
    "        index += batch_size\n",
    "        \n",
    "\n",
    "cls_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T10:57:09.764051Z",
     "iopub.status.busy": "2025-03-25T10:57:09.763732Z",
     "iopub.status.idle": "2025-03-25T10:57:09.791432Z",
     "shell.execute_reply": "2025-03-25T10:57:09.790583Z",
     "shell.execute_reply.started": "2025-03-25T10:57:09.764027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# labels, mapping = pd.factorize(df[\"Class\"])  # Vectorized encoding\n",
    "# labels = torch.tensor(labels, dtype=torch.int8, device=device)  \n",
    "df[\"Class\"] = df[\"Class\"].str.lower()  # Ensure consistent casing\n",
    "labels = torch.tensor((df[\"Class\"] != \"human\").astype(int), dtype=torch.int8, device=device)\n",
    "\n",
    "seq_IDs = df[\"seq_ID\"].to_numpy()  # OR df[\"Length\"].values\n",
    "virus_ID = df[\"virus_ID\"].to_numpy()  # OR df[\"Length\"].values\n",
    "\n",
    "torch.save(labels.cpu(), \"south_labels.pt\")\n",
    "torch.save(cls_embeddings.cpu(), \"south_embd.pt\")\n",
    "np.save(\"south_seq_IDs.npy\", seq_IDs)\n",
    "np.save(\"south_virus_ID.npy\", virus_ID)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6731611,
     "sourceId": 10881637,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6872786,
     "sourceId": 11035092,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6963613,
     "sourceId": 11160116,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
