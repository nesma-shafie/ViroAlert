{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CONSTANTS import char2idx, idx2char, max_smile_len, min_smile_len, vocab_size, device\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import Generator as Generator\n",
    "import Discriminator as Discriminator\n",
    "import torch\n",
    "from train_utilis import  load_checkpoint,pre_train, main_train\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from test_utilis import evaluate_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/kaggle/input/antivirus-generation/train_data.csv\")\n",
    "df = df[\n",
    "    df['0'].str.len().between(min_smile_len, max_smile_len-2)\n",
    "]\n",
    "drugs=df['0'].unique()\n",
    "generator = Generator(vocab_size).to(device)\n",
    "discriminator = Discriminator(vocab_size).to(device)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "g_learning_rate = 2e-4  # or 2e-4\n",
    "d_learning_rate = 1e-3  # or 2e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9  # or 0.999 depending on empirical stability\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(beta1, beta2))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "g_scaler = torch.amp.GradScaler('cuda')  \n",
    "d_scaler = torch.amp.GradScaler('cuda')  \n",
    "\n",
    "# path=\"/kaggle/input/gans-checkpoints/other/default/15/checkpoint_epoch_38.pt\"\n",
    "# start_point=load_checkpoint(generator, discriminator, g_optimizer, d_optimizer,g_scaler,d_scaler,path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f5448",
   "metadata": {},
   "source": [
    "### pre training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train(generator, discriminator,g_optimizer,d_optimizer,g_scaler, d_scaler,drugs, max_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e497e6d",
   "metadata": {},
   "source": [
    "### main training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train(generator, discriminator,g_optimizer,d_optimizer,g_scaler,d_scaler, drugs, max_len=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f013bd",
   "metadata": {},
   "source": [
    "### testing generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smiles_list=[]\n",
    "with torch.no_grad():\n",
    "    generated, _ = generator.sample(char2idx['<SOS>'], 128,10000,temperature=1)\n",
    "    for i in range(10000):\n",
    "        smiles = ''.join([idx2char[t] for t in generated[i].tolist() if t !=char2idx['<SOS>'] and t !=char2idx['<EOS>'] and t !=char2idx['<PAD>'] ])\n",
    "        smiles_list.append(smiles)\n",
    "metrics = evaluate_smiles(smiles_list, training_set=drugs)\n",
    "pprint(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
