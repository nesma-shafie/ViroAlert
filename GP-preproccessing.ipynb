{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11035092,"sourceType":"datasetVersion","datasetId":6872786}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install biopython\n!pip install python-Levenshtein","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:05.725562Z","iopub.execute_input":"2025-03-29T23:00:05.725947Z","iopub.status.idle":"2025-03-29T23:00:15.084256Z","shell.execute_reply.started":"2025-03-29T23:00:05.725914Z","shell.execute_reply":"2025-03-29T23:00:15.082817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom Bio import SeqIO\nimport os\nfrom collections import Counter\nimport torch\nfrom Levenshtein import distance\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.090498Z","iopub.execute_input":"2025-03-29T23:00:15.090947Z","iopub.status.idle":"2025-03-29T23:00:15.096962Z","shell.execute_reply.started":"2025-03-29T23:00:15.090907Z","shell.execute_reply":"2025-03-29T23:00:15.095543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def readDataFromFile(filenames):\n    dfs = []  # List to store individual DataFrames\n\n    for filename in filenames:\n        file_path = os.path.abspath(filename)  # Ensure absolute path\n        # Read and store data directly from the generator\n        df = pd.DataFrame.from_records([\n            {\n                \"ID\": \"|\".join(record.description.split(\"|\")[1:]),\n                \"Sequence\": str(record.seq),  # Extract sequence\n            }\n            for record in SeqIO.parse(file_path, \"fasta\")\n        ])\n          \n        dfs.append(df)  # Append each DataFrame\n\n    # Concatenate all DataFrames into one\n    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.098490Z","iopub.execute_input":"2025-03-29T23:00:15.098880Z","iopub.status.idle":"2025-03-29T23:00:15.120474Z","shell.execute_reply.started":"2025-03-29T23:00:15.098847Z","shell.execute_reply":"2025-03-29T23:00:15.119242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def removeDuplicateSeq(datas, ids):\n    unique_dict = {}  # Dictionary to store the last occurrence of each data\n    for data, id_ in zip(datas, ids):\n        unique_dict[data] = id_  # Overwrites previous values, keeping only the last occurrence\n    return list(unique_dict.keys()),list(unique_dict.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.121504Z","iopub.execute_input":"2025-03-29T23:00:15.121849Z","iopub.status.idle":"2025-03-29T23:00:15.141995Z","shell.execute_reply.started":"2025-03-29T23:00:15.121808Z","shell.execute_reply":"2025-03-29T23:00:15.140586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def removeDuplicateIds(datas,ids):\n    datas = np.array(datas)\n    ids = np.array(ids)\n    # Find duplicated (id, sid) pairs\n    unique_ids, counts = np.unique(ids, return_counts=True)\n    duplicate_ids = {id for id in unique_ids[counts > 1]}  # Convert to set of tuples\n    \n    \n    # Step 2: Find indices to remove\n    indices_to_remove = np.array([\n        ( id_val in duplicate_ids and 'X' in data_val) \n        for id_val, data_val in tqdm(zip(ids, datas))\n    ])\n    \n    # Remove those indices\n    filtered_data = datas[~indices_to_remove]\n    filtered_id = ids[~indices_to_remove]\n\n     # Find unique id pairs and keep only the first occurrence\n    _,unique_indices = np.unique(filtered_id, return_index=True)\n    \n    # Filter arrays\n    filtered_data = filtered_data[unique_indices]\n    filtered_id = filtered_id[unique_indices]\n\n    return filtered_data,filtered_id","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.143265Z","iopub.execute_input":"2025-03-29T23:00:15.143597Z","iopub.status.idle":"2025-03-29T23:00:15.168395Z","shell.execute_reply.started":"2025-03-29T23:00:15.143568Z","shell.execute_reply":"2025-03-29T23:00:15.166763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def removeXSeq( datas, ids):\n    # Step 1: Compute X frequencies for all sequences\n    Xfreqs = np.array([Counter(seq).get(\"X\", 0) / len(seq) for seq in datas])\n\n    # Step 2: Create a mask to filter out sequences where X frequency > 0.1\n    mask = Xfreqs == 0  # True for sequences to keep\n    # Step 3: Apply the mask to filter data\n    return (\n        np.array(datas)[mask], \n        np.array(ids)[mask]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.169522Z","iopub.execute_input":"2025-03-29T23:00:15.169878Z","iopub.status.idle":"2025-03-29T23:00:15.190953Z","shell.execute_reply.started":"2025-03-29T23:00:15.169847Z","shell.execute_reply":"2025-03-29T23:00:15.189736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def removeIncompleteSeq(datas, ids):\n    filtered_datas=[]\n    filtered_ids=[]\n    for data, id_ in zip(datas, ids):\n        if len(data) >= 100:\n            filtered_datas.append(data)\n            filtered_ids.append(id_)\n\n    return filtered_datas, filtered_ids\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.193756Z","iopub.execute_input":"2025-03-29T23:00:15.194208Z","iopub.status.idle":"2025-03-29T23:00:15.215827Z","shell.execute_reply.started":"2025-03-29T23:00:15.194154Z","shell.execute_reply":"2025-03-29T23:00:15.214480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nfrom collections import defaultdict\n\ndef hamming_distance_np(arr1, arr2):\n    \"\"\"Vectorized Hamming distance calculation using NumPy.\"\"\"\n    return np.sum(arr1 != arr2, axis=1)\n\ndef process_length_group(sequences, ids, threshold):\n    \"\"\"Processes a group of sequences of the same length and removes similar ones.\"\"\"\n    if not sequences:\n        return [], []\n    \n    # Sort sequences to prioritize those with \"human\" in their ID\n    sorted_data = sorted(zip(sequences, ids), key=lambda x: \"Human\"not in x[1])  \n    sorted_sequences, sorted_ids = zip(*sorted_data)  \n\n    unique_sequences = []\n    unique_ids = []\n\n    np_sequences = np.array([list(seq) for seq in sorted_sequences], dtype=\"<U1\")  # Convert to NumPy array\n\n    for i in tqdm(range(len(np_sequences))):\n        seq = np_sequences[i]\n        id_ = sorted_ids[i]\n\n        if not unique_sequences:\n            unique_sequences.append(seq)\n            unique_ids.append(id_)\n            continue\n\n        # Convert unique sequences to NumPy array for fast comparisons\n        np_unique = np.array(unique_sequences, dtype=\"<U1\")\n\n        # Compute Hamming distance\n        dists = hamming_distance_np(np_unique, seq)\n        normalized_dists = dists / len(seq)\n\n        # Add if it is different enough\n        if np.all(normalized_dists >= threshold):\n            unique_sequences.append(seq)\n            unique_ids.append(id_)\n\n            # if len(unique_sequences) % 100 == 0:\n            #     print(f\"Unique Sequences (Length {len(seq)}): {len(unique_sequences)}\")\n\n    # Convert back to strings\n    unique_sequences = [\"\".join(seq) for seq in unique_sequences]\n    \n    return unique_sequences, unique_ids\n\ndef remove_similar_sequences(sequences, ids, threshold=0.01/2):\n    if not sequences:\n        return [], []\n\n    # Group sequences by length\n    length_groups = defaultdict(list)\n    for seq, id_ in zip(sequences, ids):\n        length_groups[len(seq)].append((seq, id_))\n\n    # Process each group separately\n    unique_sequences = []\n    unique_ids = []\n\n    for length, seq_group in length_groups.items():\n        group_sequences, group_ids = zip(*seq_group)\n        u_seqs, u_ids = process_length_group(group_sequences, group_ids, threshold)\n        unique_sequences.extend(u_seqs)\n        unique_ids.extend(u_ids)\n\n    return unique_sequences, unique_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:15.217091Z","iopub.execute_input":"2025-03-29T23:00:15.217453Z","iopub.status.idle":"2025-03-29T23:00:15.230746Z","shell.execute_reply.started":"2025-03-29T23:00:15.217422Z","shell.execute_reply":"2025-03-29T23:00:15.229333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fileNamesOriginalDatas=[\"/kaggle/input/ncbi-viruses/africa.fa\",\n                \"/kaggle/input/ncbi-viruses/europe.fa\",\n                \"/kaggle/input/ncbi-viruses/Australia.fa\",\n                \"/kaggle/input/ncbi-viruses/asia.fa\",\n                \"/kaggle/input/ncbi-viruses/south.fa\",\n                \"/kaggle/input/ncbi-viruses/north.fa\"\n             ]\n\n\n\ndf=readDataFromFile(fileNamesOriginalDatas)\ndatas = df['Sequence'].values  # Returns a NumPy array\nids = df['ID'].values  # Returns a NumPy array\nprint(ids[0])\nprint(len(datas))\nprint(len(ids))\n\nlist_data,list_id=xremoveDuplicateSeq(datas,ids)\nlist_data,list_id=removeDuplicateIds(list_data,list_id)\nlist_data,list_id=removeXSeq(list_data,list_id)\nlist_data,list_id=removeIncompleteSeq(list_data,list_id)\nlist_data,list_id=remove_similar_sequences(list_data,list_id)\n\nprint(len(list_data))\nprint(len(list_id))\n\ndf = pd.DataFrame({\"ID\": list_id, \"Data\": list_data})\ndf.to_csv(\"ncbi_data.csv\", index=False)\n\nprint(f\"Saved {len(list_data)} seq\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:00:16.510283Z","iopub.execute_input":"2025-03-29T23:00:16.510655Z","iopub.status.idle":"2025-03-29T23:51:48.395928Z","shell.execute_reply.started":"2025-03-29T23:00:16.510621Z","shell.execute_reply":"2025-03-29T23:51:48.394815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def readDataFromfile(filename):\n    file_path = os.path.abspath(filename)  # Ensure absolute path\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n    # print(df)\n    # Rename columns for clarity\n    df.columns = [\"ID\",\"Sequence\"]\n\n    # Extract virus_ID (part after first \"|\") and seq_ID (part after second \"|\")\n    df[\"Virus_ID\"] = df[\"ID\"].apply(lambda x: \"\".join(x.split(\"|\")[1:]) if \"|\" in x else \"\")\n    df[\"Seq_ID\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[0] if \"|\" in x else \"\")\n    df[\"Class\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[-1] if \"|\" in x else \"\")\n    df[\"Length\"] = df[\"Sequence\"].apply(lambda x: len(x))\n\n    return df[[\"Sequence\",\"Virus_ID\", \"Seq_ID\", \"Class\",\"Length\"]]  # Return relevant columns\n\ndf=readDataFromfile(\"/kaggle/working/ncbi_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:51:52.799684Z","iopub.execute_input":"2025-03-29T23:51:52.800116Z","iopub.status.idle":"2025-03-29T23:51:53.645095Z","shell.execute_reply.started":"2025-03-29T23:51:52.800074Z","shell.execute_reply":"2025-03-29T23:51:53.643892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf[\"Class\"] = df[\"Class\"].str.lower()  #Ensure consistent casing\nlabels = np.array((df[\"Class\"] != \"human\").astype(int))\nids=df[\"Virus_ID\"]\nseq_ids=df[\"Seq_ID\"]+\" \"+df[\"Virus_ID\"]\n\n# convert string id to numeric\n_,ids = np.unique(ids, return_inverse=True)\n_,seq_ids = np.unique(seq_ids, return_inverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:51:53.893997Z","iopub.execute_input":"2025-03-29T23:51:53.894441Z","iopub.status.idle":"2025-03-29T23:51:54.071994Z","shell.execute_reply.started":"2025-03-29T23:51:53.894404Z","shell.execute_reply":"2025-03-29T23:51:54.070935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_human = (df[\"Class\"].str.lower() == \"human\").sum()\nnum_non_human = (df[\"Class\"].str.lower() != \"human\").sum()\nprint(\"Number of human samples:\", num_human)\nprint(\"Number of NON human samples:\", num_non_human)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:51:56.029581Z","iopub.execute_input":"2025-03-29T23:51:56.030001Z","iopub.status.idle":"2025-03-29T23:51:56.074656Z","shell.execute_reply.started":"2025-03-29T23:51:56.029965Z","shell.execute_reply":"2025-03-29T23:51:56.073452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uniq,count=np.unique(df[\"Virus_ID\"],return_counts=True)\nprint(len(uniq))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:51:57.797263Z","iopub.execute_input":"2025-03-29T23:51:57.797630Z","iopub.status.idle":"2025-03-29T23:51:57.877889Z","shell.execute_reply.started":"2025-03-29T23:51:57.797599Z","shell.execute_reply":"2025-03-29T23:51:57.876746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"human=set()\nanimal=set()\nfor virus_id,label in zip(ids,labels):\n    if (label==0):\n        human.add(virus_id)\n    else:\n        animal.add(virus_id)\nprint(len(animal))\nprint(len(human))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:51:58.384434Z","iopub.execute_input":"2025-03-29T23:51:58.384813Z","iopub.status.idle":"2025-03-29T23:51:58.428485Z","shell.execute_reply.started":"2025-03-29T23:51:58.384778Z","shell.execute_reply":"2025-03-29T23:51:58.427414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(animal)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}