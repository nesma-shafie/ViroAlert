{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-17T16:41:42.207594Z",
     "iopub.status.busy": "2025-06-17T16:41:42.207296Z",
     "iopub.status.idle": "2025-06-17T16:41:45.002384Z",
     "shell.execute_reply": "2025-06-17T16:41:45.001275Z",
     "shell.execute_reply.started": "2025-06-17T16:41:42.207571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score,roc_auc_score\n",
    "import gc\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "file_name_ncbi_datas=\"/kaggle/input/ncbi-data-csv/ncbi_cleaned_train_data.csv\"\n",
    "file_name_gasaid_datas=\"/kaggle/input/ncbi-data-csv/gasaid_cleaned_train_data.csv\"\n",
    "ncbi_test_file_name=\"/kaggle/input/ncbi-data-csv/ncbi_cleaned_test_data.csv\"\n",
    "gasaid_test_file_name=\"/kaggle/input/ncbi-data-csv/gasaid_cleaned_test_data.csv\"\n",
    "covid_test=\"/kaggle/input/ncbi-data-csv/test_covid.csv\"\n",
    "\n",
    "Asian_flu_test=\"/kaggle/input/testing-pandemics-csv/Asian Flu (1957-1958).csv\"\n",
    "hong_kong_flu_test=\"/kaggle/input/testing-pandemics-csv/Hong Kong (1968-1970).csv\"\n",
    "spanish_flu_test=\"/kaggle/input/testing-pandemics-csv/Spanish Flu (1918-1920).csv\"\n",
    "pdmh1n1_flu_test=\"/kaggle/input/testing-pandemics-csv/Swine Flu (2009-2010).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:41:45.004075Z",
     "iopub.status.busy": "2025-06-17T16:41:45.003445Z",
     "iopub.status.idle": "2025-06-17T16:42:38.645677Z",
     "shell.execute_reply": "2025-06-17T16:42:38.644861Z",
     "shell.execute_reply.started": "2025-06-17T16:41:45.004049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "# from gensim.models import Word2Vec,KeyedVectors\n",
    "from gensim.models import FastText,KeyedVectors\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Auto-detect GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.647160Z",
     "iopub.status.busy": "2025-06-17T16:42:38.646519Z",
     "iopub.status.idle": "2025-06-17T16:42:38.652882Z",
     "shell.execute_reply": "2025-06-17T16:42:38.651794Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.647131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# need validation\n",
    "N=193\n",
    "SG_EMBEDD_SIZE=30\n",
    "SG_WINDOW=5\n",
    "# Transformer Parameters\n",
    "N_HEAD = 5         # Number of attention heads\n",
    "ENCODER_N_LAYERS = 2       # Number of transformer layers\n",
    "EMBEDDING_SIZE=SG_EMBEDD_SIZE\n",
    "INTERMIDIATE_DIM=512\n",
    "BATCH_SIZE=32\n",
    "\n",
    "l_sub=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.655505Z",
     "iopub.status.busy": "2025-06-17T16:42:38.655133Z",
     "iopub.status.idle": "2025-06-17T16:42:38.672470Z",
     "shell.execute_reply": "2025-06-17T16:42:38.671582Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.655480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ASW(sequence, l_sub):\n",
    "    \"\"\"\n",
    "        sequence (str): The original viral sequence.\n",
    "        l_sub (int): The length of each subsequence.\n",
    "        n (int): The number of subsequences to generate.\n",
    "    \"\"\"\n",
    "    l = len(sequence)\n",
    "    \n",
    " \n",
    "    if N> 1:\n",
    "        l_stride = (l - l_sub) // (N - 1)\n",
    "    else:\n",
    "        l_stride = 1  \n",
    "    \n",
    "    subsequences = []\n",
    "\n",
    " \n",
    "    for i in range(0, min(N * l_stride, l - l_sub + 1), l_stride):\n",
    "        subsequences.append(sequence[i:i + l_sub])\n",
    "    \n",
    "    return subsequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.673699Z",
     "iopub.status.busy": "2025-06-17T16:42:38.673438Z",
     "iopub.status.idle": "2025-06-17T16:42:38.696731Z",
     "shell.execute_reply": "2025-06-17T16:42:38.695865Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.673677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 0->human  1-> animals\n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self,N_HEAD,ENCODER_N_LAYERS,EMBEDDING_SIZE,INTERMIDIATE_DIM):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.M = EMBEDDING_SIZE\n",
    "        self.L = INTERMIDIATE_DIM\n",
    "        self.ENCODER_N_LAYERS=ENCODER_N_LAYERS\n",
    "        self.ATTENTION_BRANCHES = 1\n",
    "        self.N_HEAD=N_HEAD\n",
    "\n",
    "        # embedding \n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=self.M, nhead=self.N_HEAD)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=self.ENCODER_N_LAYERS)\n",
    "        \n",
    "        # instance level \n",
    "        self.attention_V_1 = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U_1 = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix U\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_w_1 = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "\n",
    "\n",
    "        # bag level \n",
    "        self.attention_V_2 = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U_2 = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix U\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_w_2 = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "\n",
    "\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.ATTENTION_BRANCHES, out_channels=128, kernel_size=4, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "            nn.Flatten(),  # Converts to 1D before fully connected layers\n",
    "            nn.Linear(128 * ((self.M) // 4), 256),  # Adjust size based on sequence length\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, datas,ids,Seq_ids):\n",
    "        A_vec_2_all=[]\n",
    "        A_vec_all=[]\n",
    "        #### STEP 1:embeddings\n",
    "        datas = datas.float()  # Ensure correct dtype\n",
    "        instances=self.transformer_encoder(datas) \n",
    "        \n",
    "        #### STEP 2: INSTANCE-LEVEL ATTENTION ####\n",
    "        # Apply attention mechanisms per bag (over instances_per_bag)\n",
    "        A_V = self.attention_V_1(instances)  \n",
    "        A_U = self.attention_U_1(instances)  \n",
    "        A = self.attention_w_1(A_V * A_U)\n",
    "        A = torch.transpose(A, 1, 0)  \n",
    "        inner_bags = torch.unique_consecutive(Seq_ids)\n",
    "      \n",
    "        output = torch.empty(((len(inner_bags), self.M))).to(device)\n",
    "        super_ids = torch.empty(((len(inner_bags))))\n",
    "        for i, bag in enumerate(inner_bags):\n",
    "            A_vec=F.softmax(A[0][Seq_ids == bag],dim=0)\n",
    "            A_vec_all.append(A_vec)\n",
    "            output[i] = torch.matmul(A_vec, instances[Seq_ids == bag])\n",
    "            super_ids[i]=ids[Seq_ids == bag][0]\n",
    "        \n",
    "        ### STEP 3: BAG-LEVEL ATTENTION ####\n",
    "        A_V_2 = self.attention_V_2(output)  \n",
    "        A_U_2 = self.attention_U_2(output)  \n",
    "        A_2 = self.attention_w_2(A_V_2 * A_U_2)  \n",
    "        A_2 = torch.transpose(A_2, 1,0)   \n",
    "\n",
    "      \n",
    "        outer_bags = torch.unique_consecutive(super_ids)\n",
    "        output2 = torch.empty(((len(outer_bags), self.M))).to(device)\n",
    "\n",
    "        for i, bag in enumerate(outer_bags):\n",
    "            A_vec_2=F.softmax(A_2[0][super_ids == bag],dim=0)\n",
    "            A_vec_2_all.append(A_vec_2)\n",
    "            output2[i] = torch.matmul(A_vec_2, output[super_ids == bag])\n",
    "\n",
    "        \n",
    "        \n",
    "        ### STEP 4: CLASSIFICATION ####\n",
    "        # output2 = output2.view(output2.shape[0], -1)  # Flatten over bags_per_bag for classification\n",
    "        output2 = output2.unsqueeze(1)  # Add a channel dimension\n",
    "\n",
    "\n",
    "        Y_prob = self.classifier(output2)  # Shape: [batch_size, 1]\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()  # Convert probabilities to binary predictions\n",
    "        return Y_prob, Y_hat, A_vec_all, A_vec_2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.698187Z",
     "iopub.status.busy": "2025-06-17T16:42:38.697901Z",
     "iopub.status.idle": "2025-06-17T16:42:38.720841Z",
     "shell.execute_reply": "2025-06-17T16:42:38.719876Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.698165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MILDataset(Dataset):\n",
    "    def __init__(self, datas, ids, seq_ids, labels):\n",
    "        self.datas = datas  # Instance features\n",
    "        self.ids = ids # Virus (outer bag) IDs\n",
    "        self.seq_ids = seq_ids  # Sequence (inner bag) IDs\n",
    "        self.labels = labels.to(\"cpu\")  # Labels at the virus (outer bag) level\n",
    "\n",
    "        # Unique IDs for outer bags (viruses) and their indices\n",
    "        self.unique_virus_ids, self.virus_indices = torch.unique(self.ids, return_inverse=True)\n",
    "        \n",
    "        # Unique IDs for inner bags (sequences) and their indices\n",
    "        self.unique_seq_ids, self.seq_indices = torch.unique(self.seq_ids, return_inverse=True)\n",
    "\n",
    "        # Mapping from virus to instance indices  2d array each list is the virus data indecies\n",
    "        self.virus_bag_indices_list = [torch.where(self.virus_indices == i)[0].to(\"cpu\") for i in tqdm(range(len(self.unique_virus_ids)))]\n",
    "\n",
    "        # Mapping from sequence to instance indices 2d array each list is the seq data indecies\n",
    "        self.seq_bag_indices_list = [torch.where(self.seq_indices == i)[0].to(\"cpu\") for i in tqdm( range(len(self.unique_seq_ids)))]\n",
    "\n",
    "        # Labels assigned at the virus level (each virus gets one label)\n",
    "        self.virus_labels = [self.labels[indices[0]] for indices in self.virus_bag_indices_list]\n",
    "\n",
    "        # Precomputed bag-of-bags structure (virus → [seq])\n",
    "        self.virus_seq_map = {}  # Maps virus_id -> list of sequence indices\n",
    "        for i, virus_id in tqdm(enumerate(self.unique_virus_ids)):\n",
    "            self.virus_seq_map[virus_id.item()] = list((self.seq_ids[self.virus_bag_indices_list[i]].tolist()))\n",
    "\n",
    "        # Precomputed bag IDs for each virus and sequence\n",
    "        self.precomputed_virus_ids = [torch.full((indices.shape[0],), self.unique_virus_ids[i], dtype=torch.long) \n",
    "                                      for i, indices in enumerate(self.virus_bag_indices_list)]\n",
    "\n",
    "      \n",
    "        self.datas = self.datas.cpu()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_virus_ids)  # Number of unique viruses (outer bags)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Return outer bag (virus), inner bags (sequences), and instance-level data. \"\"\"\n",
    "        \n",
    "        # Get all instance indices belonging to this virus\n",
    "        virus_instance_indices = self.virus_bag_indices_list[index]\n",
    "        # Retrieve instance-level data\n",
    "        virus_data = self.datas[virus_instance_indices]\n",
    "        virus_label = self.virus_labels[index]\n",
    "        virus_id = self.precomputed_virus_ids[index]\n",
    "        # Find which sequences belong to this virus\n",
    "       \n",
    "        seq_ids_in_virus = self.virus_seq_map[virus_id[0].item()]\n",
    "\n",
    "        return {\n",
    "            \"virus_id\": virus_id,\n",
    "            \"virus_data\": virus_data,\n",
    "            \"virus_label\": virus_label,\n",
    "            \"seq_id\": seq_ids_in_virus\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" Custom collate function for Bag-of-Bags MIL \"\"\"\n",
    "\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    all_virus_ids = []\n",
    "    all_virus_data = []\n",
    "    all_virus_labels = []\n",
    "    all_virus_seq_ids = []\n",
    "   \n",
    "\n",
    "    for item in batch:\n",
    "        virus_id = item[\"virus_id\"].tolist()\n",
    "        virus_data = item[\"virus_data\"].tolist()\n",
    "        virus_label = item[\"virus_label\"]\n",
    "        seq_id = item[\"seq_id\"]\n",
    "\n",
    "        all_virus_seq_ids.extend(seq_id)\n",
    "        all_virus_ids.extend(virus_id)\n",
    "        all_virus_data.extend(virus_data)\n",
    "        all_virus_labels.append(virus_label)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    batch_virus_labels = torch.tensor(all_virus_labels, dtype=torch.float)\n",
    "    batch_seq_ids = torch.tensor(all_virus_seq_ids, dtype=torch.float)\n",
    "    batch_virus_datas = torch.tensor(all_virus_data, dtype=torch.float)\n",
    "    batch_virus_ids = torch.tensor(all_virus_ids, dtype=torch.float)\n",
    "\n",
    "\n",
    "    return batch_virus_datas, batch_virus_ids,batch_seq_ids, batch_virus_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.722059Z",
     "iopub.status.busy": "2025-06-17T16:42:38.721719Z",
     "iopub.status.idle": "2025-06-17T16:42:38.742514Z",
     "shell.execute_reply": "2025-06-17T16:42:38.741474Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.722030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_deep(dataloader):\n",
    "    # print(len(labels))\n",
    "    model.eval()\n",
    "    acc=0\n",
    "    total_samples=0\n",
    "    output=[]\n",
    "    probs = []\n",
    "    true_lables=[]\n",
    "    with torch.no_grad():\n",
    "         for batch_data, batch_ids,batch_seq_ids, batch_labels in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "            batch_data,batch_ids,batch_seq_ids, batch_labels  = batch_data.to(device), batch_ids.to(device),batch_seq_ids.to(device), batch_labels.to(device)\n",
    "            Y_prob, Y_hat, A, A_2 =model(batch_data,batch_ids,batch_seq_ids)         \n",
    "            output += Y_hat.cpu().tolist()\n",
    "            true_lables += batch_labels.cpu().tolist()\n",
    "            # Y_prob=Y_prob.squeeze(1)\n",
    "            probs += Y_prob.squeeze(1).cpu().tolist() \n",
    "            Y_hat = Y_hat.view_as(batch_labels)\n",
    "            acc += ((Y_hat == batch_labels).sum().item())\n",
    "            total_samples += batch_labels.size(0)  # Track the total number of samples processed\n",
    "\n",
    "    acc=acc/total_samples*100\n",
    "    print(f'acc: {acc:.1f}%')\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_lables, output, target_names=[\"Human\", \"Animal\"]))\n",
    "\n",
    "    con_matrix(true_lables, output)\n",
    "    # plot_roc_curve(true_lables, output)\n",
    "    return probs,true_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.743748Z",
     "iopub.status.busy": "2025-06-17T16:42:38.743499Z",
     "iopub.status.idle": "2025-06-17T16:42:38.762906Z",
     "shell.execute_reply": "2025-06-17T16:42:38.761986Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.743725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_file(filename):\n",
    "    file_path = os.path.abspath(filename)\n",
    "\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = [\"ID\", \"Sequence\"]\n",
    "    \n",
    "    # Drop rows where Sequence is not a string or is missing\n",
    "    df = df[df[\"Sequence\"].apply(lambda x: isinstance(x, str))].copy()\n",
    "    df = df[df[\"Sequence\"].apply(lambda x: len(x) > 200 if isinstance(x, str) else False)].copy()\n",
    "    df = df[~df['Sequence'].str.contains(r'[^ACDEFGHIKLMNPQRSTVWY]')]\n",
    "\n",
    "    # Extract fields\n",
    "    df[\"Virus_ID\"] = df[\"ID\"].apply(lambda x: \"\".join(x.split(\"|\")[1:]) if \"|\" in x else \"\")\n",
    "    df[\"Seq_ID\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[0] if \"|\" in x else \"\")\n",
    "    df[\"Class\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[-1] if \"|\" in x else \"\")\n",
    "    df[\"Length\"] = df[\"Sequence\"].apply(len)\n",
    "\n",
    "    return df[[\"Sequence\", \"Virus_ID\", \"Seq_ID\", \"Class\", \"Length\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.764202Z",
     "iopub.status.busy": "2025-06-17T16:42:38.763933Z",
     "iopub.status.idle": "2025-06-17T16:42:38.782944Z",
     "shell.execute_reply": "2025-06-17T16:42:38.781988Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.764179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_common_sequences(df1, df2, column='Sequence'):\n",
    "    common_mask = df1[column].isin(df2[column])\n",
    "    if common_mask.any():\n",
    "        count = common_mask.sum()\n",
    "        print(f\"{count} common sequences found. Removing them from df2...\")\n",
    "        df1_cleaned = df1[~common_mask].copy()\n",
    "    else:\n",
    "        print(\"No common sequences found between the two DataFrames.\")\n",
    "        df1_cleaned = df1.copy()\n",
    "\n",
    "    return df1_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.786967Z",
     "iopub.status.busy": "2025-06-17T16:42:38.786620Z",
     "iopub.status.idle": "2025-06-17T16:42:38.798340Z",
     "shell.execute_reply": "2025-06-17T16:42:38.797463Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.786945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df, subset=None, show_duplicates=True):\n",
    "    duplicated_mask = df.duplicated(subset=subset)\n",
    "    has_duplicates = duplicated_mask.any()\n",
    "\n",
    "    if has_duplicates:\n",
    "        num_duplicates = duplicated_mask.sum()\n",
    "        print(f\"⚠️ Found {num_duplicates} duplicated row(s). Removing them...\")\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "    else:\n",
    "        print(\"✅ No duplicated rows found.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.799930Z",
     "iopub.status.busy": "2025-06-17T16:42:38.799196Z",
     "iopub.status.idle": "2025-06-17T16:42:38.816198Z",
     "shell.execute_reply": "2025-06-17T16:42:38.815265Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.799896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply ASW \n",
    "def create_data_loader(datas,labels,ids,seq_ids,ft_model):\n",
    "    embeddings, ids,seq_ids, labels=tokenize(datas,ids,seq_ids,labels,ft_model)\n",
    "    mildataset = MILDataset(embeddings, ids,seq_ids, labels)\n",
    "    data_loader = DataLoader(mildataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:42:38.817385Z",
     "iopub.status.busy": "2025-06-17T16:42:38.817045Z",
     "iopub.status.idle": "2025-06-17T16:43:07.233159Z",
     "shell.execute_reply": "2025-06-17T16:43:07.231919Z",
     "shell.execute_reply.started": "2025-06-17T16:42:38.817357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = GatedAttention(N_HEAD, ENCODER_N_LAYERS, EMBEDDING_SIZE, INTERMIDIATE_DIM).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/virogen-model/pytorch/default/1/model_weights.pth\", map_location=device))\n",
    "model.eval()\n",
    "ft_model = FastText.load(\"/kaggle/input/virogen-model/pytorch/default/1/ft_skipgram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.234708Z",
     "iopub.status.busy": "2025-06-17T16:43:07.234289Z",
     "iopub.status.idle": "2025-06-17T16:43:07.473870Z",
     "shell.execute_reply": "2025-06-17T16:43:07.472539Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.234676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test_gasaid=read_data_from_file(gasaid_test_file_name)\n",
    "df_test_ncbi=read_data_from_file(ncbi_test_file_name)\n",
    "\n",
    "df_test = pd.concat([df_test_gasaid, df_test_ncbi], ignore_index=True)\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.475125Z",
     "iopub.status.busy": "2025-06-17T16:43:07.474843Z",
     "iopub.status.idle": "2025-06-17T16:43:07.504097Z",
     "shell.execute_reply": "2025-06-17T16:43:07.502370Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.475105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test = remove_duplicates(df_test)\n",
    "\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.505469Z",
     "iopub.status.busy": "2025-06-17T16:43:07.505159Z",
     "iopub.status.idle": "2025-06-17T16:43:07.643081Z",
     "shell.execute_reply": "2025-06-17T16:43:07.642022Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.505447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_Asian_flu=read_data_from_file(Asian_flu_test)\n",
    "df_hong_kong_flu=read_data_from_file(hong_kong_flu_test)\n",
    "df_pdmh1n1_flu=read_data_from_file(pdmh1n1_flu_test)\n",
    "df_covid= read_data_from_file(covid_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.644153Z",
     "iopub.status.busy": "2025-06-17T16:43:07.643910Z",
     "iopub.status.idle": "2025-06-17T16:43:07.650364Z",
     "shell.execute_reply": "2025-06-17T16:43:07.649406Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.644134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(df_Asian_flu))\n",
    "print(len(df_hong_kong_flu))\n",
    "print(len(df_pdmh1n1_flu))\n",
    "print(len(df_covid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.651469Z",
     "iopub.status.busy": "2025-06-17T16:43:07.651235Z",
     "iopub.status.idle": "2025-06-17T16:43:07.673456Z",
     "shell.execute_reply": "2025-06-17T16:43:07.672375Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.651451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 4  # k-mer size\n",
    "m = 3  # minimizer size\n",
    "pseudo_count = 0.1\n",
    "alphabet = ['A','C','D', 'E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "alphabet = sorted(set(alphabet))\n",
    "alphabet_size = len(alphabet)\n",
    "char_to_idx = {c: i for i, c in enumerate(alphabet)}\n",
    "alphabet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.674547Z",
     "iopub.status.busy": "2025-06-17T16:43:07.674295Z",
     "iopub.status.idle": "2025-06-17T16:43:07.697476Z",
     "shell.execute_reply": "2025-06-17T16:43:07.696460Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.674527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_all_combinations(alphabet, k):\n",
    "    return [''.join(p) for p in product(alphabet, repeat=k)]\n",
    "\n",
    "# All the m-mers (not k-mers!) combinations\n",
    "combos = get_all_combinations(alphabet, m)\n",
    "combo_index = {mer: i for i, mer in enumerate(combos)}  # Faster lookup\n",
    "\n",
    "def get_kmers(seq, k):\n",
    "    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "\n",
    "def get_minimizer(kmer, m):\n",
    "    \"\"\"\n",
    "    Get lex smallest m-mer in both the k-mer and its reverse.\n",
    "    \"\"\"\n",
    "    kmer_rev = kmer[::-1]\n",
    "    all_mmers = [kmer[i:i+m] for i in range(len(kmer) - m + 1)]\n",
    "    all_mmers += [kmer_rev[i:i+m] for i in range(len(kmer_rev) - m + 1)]\n",
    "    return min(all_mmers)\n",
    "\n",
    "def comp_minimizers(seq, k, m):\n",
    "    \"\"\"Return list of minimizers for k-mers in sequence\"\"\"\n",
    "    kmers = get_kmers(seq, k)\n",
    "    return [get_minimizer(kmer, m) for kmer in kmers]\n",
    "\n",
    "def get_alphabet_count(col, alphabet):\n",
    "    \"\"\"Return count vector of characters in one column of matrix A\"\"\"\n",
    "    counter = Counter(col)\n",
    "    return np.array([counter.get(char, 0) for char in alphabet])\n",
    "\n",
    "def get_p_c():\n",
    "    # Codon-based number of mappings to amino acids\n",
    "    codon_table = {\n",
    "        'A': 4, 'C': 2, 'D': 2, 'E': 2, 'F': 2, 'G': 4,\n",
    "        'H': 2, 'I': 3, 'K': 2, 'L': 6, 'M': 1, 'N': 2,\n",
    "        'P': 4, 'Q': 2, 'R': 6, 'S': 6, 'T': 4, 'V': 4,\n",
    "        'W': 1, 'Y': 2\n",
    "    }\n",
    "    # Return vector for p(c) aligned with the alphabet\n",
    "    return np.array([codon_table.get(c, 1) / 61 for c in alphabet])\n",
    "\n",
    "def comp_ppm(pfm):\n",
    "    # Step 1: Add pseudocounts\n",
    "    pfm = pfm + pseudo_count\n",
    "    # Step 2: Compute PPM with small constant to avoid division by zero\n",
    "    return (pfm / (np.sum(pfm, axis=0) + 1e-9))\n",
    "    \n",
    "def comp_pwm(pfm, p_c):\n",
    "    # Step 1: Add pseudocounts\n",
    "    pfm = pfm + pseudo_count\n",
    "    # Step 2: Compute PPM with small constant to avoid division by zero\n",
    "    ppm = pfm / (np.sum(pfm, axis=0) + 1e-9)\n",
    "    # Step 3: Compute log-odds PWM\n",
    "    return np.log2(ppm / p_c[:, np.newaxis])\n",
    "\n",
    "def comp_mmers_score(mmer, pwm):\n",
    "    # Score is sum of weights for each character position\n",
    "    # W(\"AC\") = W['A'][0] + W['C'][1]\n",
    "    return sum(pwm[char_to_idx[c], i] for i, c in enumerate(mmer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.698880Z",
     "iopub.status.busy": "2025-06-17T16:43:07.698583Z",
     "iopub.status.idle": "2025-06-17T16:43:07.720850Z",
     "shell.execute_reply": "2025-06-17T16:43:07.719707Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.698857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pfm = np.zeros((alphabet_size, m))\n",
    "v = np.zeros(len(combos))\n",
    "def Pwm2Vec(S, alphabet=alphabet, k=k, m=m):\n",
    "    V = np.zeros((len(S), (alphabet_size*m)), dtype=np.float32)\n",
    "    for j, seq  in enumerate(tqdm(S, desc=\"Processing sequences\")):\n",
    "        A = comp_minimizers(seq, k, m)  # List of minimizers (m-mers)\n",
    "\n",
    "        pfm.fill(0)\n",
    "        for i in range(m):\n",
    "            # the chars in pos i in A\n",
    "            col = [a[i] for a in A]\n",
    "            # PFM[c][i] = count of character c at position i across all m-mers\n",
    "            pfm[:, i] = get_alphabet_count(col, alphabet)\n",
    "        \n",
    "        ppm = comp_ppm(pfm)\n",
    "        \n",
    "        V[j, :] = ppm.flatten()  # Assign the computed vector directly to row j\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.722320Z",
     "iopub.status.busy": "2025-06-17T16:43:07.722034Z",
     "iopub.status.idle": "2025-06-17T16:43:07.742609Z",
     "shell.execute_reply": "2025-06-17T16:43:07.741660Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.722300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pfm = np.zeros((alphabet_size, m))\n",
    "v = np.zeros(len(combos))\n",
    "def Virus2Vec(S, alphabet=alphabet, k=k, m=m):\n",
    "    V = np.zeros((len(S), len(combos)), dtype=np.float32)\n",
    "    for j, seq  in enumerate(tqdm(S, desc=\"Processing sequences\")):\n",
    "        A = comp_minimizers(seq, k, m)  # List of minimizers (m-mers)\n",
    "\n",
    "        pfm.fill(0)\n",
    "        for i in range(m):\n",
    "            # the chars in pos i in A\n",
    "            col = [a[i] for a in A]\n",
    "            # PFM[c][i] = count of character c at position i across all m-mers\n",
    "            pfm[:, i] = get_alphabet_count(col, alphabet)\n",
    "        \n",
    "        p_c = get_p_c()\n",
    "        pwm = comp_pwm(pfm, p_c)\n",
    "        \n",
    "        # Step 4: Score for all Minimizers\n",
    "        W   = [comp_mmers_score(mmer, pwm) for mmer in A]\n",
    "        # Step 5\n",
    "        v.fill(0) # feature vector of size |Σ|^m\n",
    "        for i in range(len(A)):    # A contains minimizers (m-mers)\n",
    "            idx = combo_index[A[i]]  # find index of the i-th m-mer\n",
    "            v[idx] += W[i]  # add the minimizer's score\n",
    "        V[j, :] = v  # Assign the computed vector directly to row j\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.744191Z",
     "iopub.status.busy": "2025-06-17T16:43:07.743891Z",
     "iopub.status.idle": "2025-06-17T16:43:07.761597Z",
     "shell.execute_reply": "2025-06-17T16:43:07.760590Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.744168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hydrophobicity (h1) and Hydrophilicity (h2)\n",
    "hydrophobicity = {\n",
    "    'A': 0.62,  'C': 0.29,  'D': -0.90, 'E': -0.74, 'F': 1.19,\n",
    "    'G': 0.48,  'H': -0.40, 'I': 1.38,  'K': -1.50, 'L': 1.06,\n",
    "    'M': 0.64,  'N': -0.78, 'P': 0.12,  'Q': -0.85, 'R': -2.53,\n",
    "    'S': -0.18, 'T': -0.05, 'V': 1.08,  'W': 0.81,  'Y': 0.26\n",
    "}\n",
    "\n",
    "hydrophilicity = {\n",
    "    'A': -0.50, 'C': -1.00, 'D': 3.00,  'E': 3.00,  'F': -2.50,\n",
    "    'G': 0.00,  'H': -0.50, 'I': -1.80, 'K': 3.00,  'L': -1.80,\n",
    "    'M': -1.30, 'N': 0.20,  'P': 0.00,  'Q': 0.20,  'R': 3.00,\n",
    "    'S': 0.30,  'T': -0.40, 'V': -1.50, 'W': -3.40, 'Y': -2.30\n",
    "}\n",
    "\n",
    "amino_acids = ['A','C','D', 'E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.762878Z",
     "iopub.status.busy": "2025-06-17T16:43:07.762599Z",
     "iopub.status.idle": "2025-06-17T16:43:07.785699Z",
     "shell.execute_reply": "2025-06-17T16:43:07.784534Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.762858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#30\n",
    "def apaac(sequence, lambda_value=10, weight=0.05):\n",
    "    \"\"\"\n",
    "    Feature APAAC (Amphiphilic Pseudo Amino Acid Composition)\n",
    "\n",
    "    This function computes the APAAC feature vector for a given protein sequence.\n",
    "    APAAC extends the standard amino acid composition (AAC) by incorporating \n",
    "    sequence-order information based on two physicochemical properties:\n",
    "    hydrophobicity and hydrophilicity.\n",
    "\n",
    "    Components:\n",
    "    - AAC: Frequency of each of the 20 amino acids in the sequence.\n",
    "    - Lambda correlation factors: Capture local sequence-order information \n",
    "      using pairwise differences in hydrophobicity and hydrophilicity for residues \n",
    "      separated by 1 to `lambda_value` positions.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (str): Protein sequence using one-letter amino acid codes.\n",
    "    - lambda_value (int): Number of sequence-order correlation factors to compute.\n",
    "    - weight (float): Weighting factor that controls the influence of sequence-order info.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Concatenated feature vector of length 20 + lambda_value\n",
    "    \"\"\"\n",
    "    sequence = sequence.upper()  \n",
    "    \n",
    "    # Compute standard amino acid composition (AAC)\n",
    "    aac = np.array([sequence.count(aa) / len(sequence) for aa in amino_acids])\n",
    "\n",
    "    # Compute sequence-order correlation factors\n",
    "    lambda_correlation = np.zeros(lambda_value)\n",
    "    \n",
    "    for i in range(1, lambda_value + 1):\n",
    "        sum_corr = 0\n",
    "        for j in range(len(sequence) - i):\n",
    "            h1_corr = (hydrophobicity[sequence[j]] - hydrophobicity[sequence[j + i]])**2\n",
    "            h2_corr = (hydrophilicity[sequence[j]] - hydrophilicity[sequence[j + i]])**2\n",
    "            sum_corr += (h1_corr + h2_corr) / 2  # Average correlation\n",
    "        lambda_correlation[i-1] = ((sum_corr+1e-7) / ((len(sequence) - i)+1e-7))\n",
    "\n",
    "    # Normalize and combine features\n",
    "    return np.concatenate((aac * (1 - weight * sum(lambda_correlation)), weight * lambda_correlation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.787413Z",
     "iopub.status.busy": "2025-06-17T16:43:07.787070Z",
     "iopub.status.idle": "2025-06-17T16:43:07.808945Z",
     "shell.execute_reply": "2025-06-17T16:43:07.807659Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.787385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "secondary_structure = {\n",
    "\"Helix\": set(\"EALMQKRH\"),\n",
    "\"Strand\": set(\"VIYCWFT\"),\n",
    "\"Coil\": set(\"GNPSD\"),\n",
    "}\n",
    "\n",
    "#15\n",
    "def ctdd(sequence):\n",
    "    \"\"\"\n",
    "    Feature CTDD (Distribution Descriptor of Amino Acid Properties)\n",
    "\n",
    "    This function calculates the \"Distribution\" part of the CTD (Composition, \n",
    "    Transition, Distribution) descriptor, which describes how amino acids \n",
    "    belonging to specific physicochemical property classes are distributed \n",
    "    across a protein sequence.\n",
    "\n",
    "    For each amino acid class (e.g., based on secondary structure propensity),\n",
    "    it computes five percentile-based positional features:\n",
    "    - The relative positions (normalized by sequence length) of the first,\n",
    "      25%, 50%, 75%, and last occurrence of any amino acid in that class.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (str): Protein sequence using one-letter amino acid codes.\n",
    "\n",
    "    Returns:\n",
    "    - list: A feature vector of length 15 values\n",
    "    \"\"\"\n",
    "    ctdd_vector = []\n",
    "    sequence_length=len(sequence)\n",
    "    for class_name, amino_acids in secondary_structure.items():\n",
    "        positions = [i for i, aa in enumerate(sequence) if aa in amino_acids]\n",
    "        \n",
    "        if not positions:  # If no amino acid of this class is found\n",
    "            ctdd_vector.extend([0, 0, 0, 0, 0])\n",
    "            continue\n",
    "\n",
    "        # Calculate the five key positions (first, 25%, 50%, 75%, last)\n",
    "        first = positions[0] / sequence_length\n",
    "        p25 = positions[int(len(positions) * 0.25)] / sequence_length\n",
    "        p50 = positions[int(len(positions) * 0.50)] / sequence_length\n",
    "        p75 = positions[int(len(positions) * 0.75)] / sequence_length\n",
    "        last = positions[-1] / sequence_length\n",
    "\n",
    "        ctdd_vector.extend([first, p25, p50, p75, last])\n",
    "\n",
    "    return ctdd_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.810386Z",
     "iopub.status.busy": "2025-06-17T16:43:07.810077Z",
     "iopub.status.idle": "2025-06-17T16:43:07.833143Z",
     "shell.execute_reply": "2025-06-17T16:43:07.831939Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.810364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Grpups based on their dipoles and side-chain volumes\n",
    "amino_acid_groups = {\n",
    "'A': 1, 'G': 1, 'V': 1,  # Group 1\n",
    "'I': 2, 'L': 2, 'F': 2, 'P': 2,  # Group 2\n",
    "'Y': 3, 'M': 3, 'T': 3, 'S': 3,  # Group 3\n",
    "'H': 4, 'N': 4, 'Q': 4, 'W': 4,  # Group 4\n",
    "'R': 5, 'K': 5,  # Group 5\n",
    "'D': 6, 'E': 6,  # Group 6\n",
    "'C': 7   # Group 7\n",
    " }\n",
    "\n",
    "#343\n",
    "def ctriad(sequence):\n",
    "    \"\"\"\n",
    "    Feature #343: Conjoint Triad (CTriad)\n",
    "\n",
    "    This function computes the Conjoint Triad (CTriad) feature vector for a protein sequence.\n",
    "    It maps amino acids into 7 predefined groups based on dipole moments and side-chain volumes,\n",
    "    then extracts all overlapping triads (3-residue sliding windows) and counts the frequency\n",
    "    of each of the 343 possible group combinations (7 × 7 × 7 = 343).\n",
    "\n",
    "    Components:\n",
    "    - Reduces each amino acid to one of 7 groups.\n",
    "    - Forms triads (triplets) using a sliding window across the grouped sequence.\n",
    "    - Counts and normalizes the frequency of each triad pattern.\n",
    "    - Produces a 343-dimensional feature vector representing all triad combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (str): Protein sequence using one-letter amino acid codes.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Flattened 343-dimensional vector of normalized triad frequencies.\n",
    "    \"\"\"\n",
    "    # Convert sequence to reduced alphabet (group numbers)\n",
    "    reduced_seq = [amino_acid_groups[aa] - 1 for aa in sequence if aa in amino_acid_groups]\n",
    "    # Extract triads\n",
    "    triads = [tuple(reduced_seq[i:i+3]) for i in range(len(reduced_seq) - 2)]\n",
    "   \n",
    "    # Count occurrences of each triad\n",
    "    triad_counts = Counter(triads)\n",
    "\n",
    "    # Normalize counts\n",
    "    total_triads = len(triads)\n",
    "    triad_vector = np.zeros((7, 7, 7))  # 7^3 possible triads\n",
    "\n",
    "    for triad, count in triad_counts.items():\n",
    "        triad_vector[triad] = count / total_triads  # Normalize frequency\n",
    "\n",
    "    return triad_vector.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.834421Z",
     "iopub.status.busy": "2025-06-17T16:43:07.834095Z",
     "iopub.status.idle": "2025-06-17T16:43:07.851521Z",
     "shell.execute_reply": "2025-06-17T16:43:07.850285Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.834392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#400\n",
    "def dde(sequence):\n",
    "    \"\"\"\n",
    "    Feature #400: DDE (Dipeptide Deviation from Expected Mean)\n",
    "\n",
    "    This function calculates the DDE (Dipeptide Deviation from Expected mean) descriptor\n",
    "    for a given protein sequence. It evaluates how the observed frequency of each possible\n",
    "    dipeptide deviates from its expected frequency under the assumption of amino acid independence.\n",
    "\n",
    "    Components:\n",
    "    - Dc: Observed frequency of each of the 400 dipeptides (20 × 20).\n",
    "    - Tm: Theoretical mean frequency of each dipeptide (product of individual amino acid frequencies).\n",
    "    - Tv: Theoretical variance assuming independent occurrence.\n",
    "    - DDE: Z-score-like value capturing standardized deviation (Dc - Tm) / sqrt(Tv)\n",
    "\n",
    "    This descriptor is useful in capturing **non-random pairwise amino acid associations** \n",
    "    and provides insight into sequence-specific dipeptide usage biases.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (str): Protein sequence using one-letter amino acid codes.\n",
    "\n",
    "    Returns:\n",
    "    - list: A 400-dimensional feature vector representing the DDE of all possible dipeptides.\n",
    "    \"\"\"\n",
    "    \n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'  \n",
    "    dipeptides = [''.join(pair) for pair in itertools.product(amino_acids, repeat=2)]\n",
    "    \n",
    "    aa_counts = Counter(sequence)\n",
    "    L = len(sequence)\n",
    "    aa_freq = {aa: aa_counts.get(aa, 0) / L for aa in amino_acids}\n",
    "\n",
    "    dipeptide_counts = Counter([sequence[i:i+2] for i in range(L-1)])\n",
    "    Dc = {dp: dipeptide_counts.get(dp, 0) / (L-1) for dp in dipeptides}\n",
    "\n",
    "    Tm = {dp: aa_freq[dp[0]] * aa_freq[dp[1]] for dp in dipeptides}\n",
    "    Tv = {dp: (Tm[dp] * (1 - Tm[dp])) / L if Tm[dp] > 0 else 0 for dp in dipeptides}\n",
    "\n",
    "    DDE = {dp: (Dc[dp] - Tm[dp]) / (Tv[dp] ** 0.5) if Tv[dp] > 0 else 0 for dp in dipeptides}\n",
    "\n",
    "    return list(DDE.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.852741Z",
     "iopub.status.busy": "2025-06-17T16:43:07.852411Z",
     "iopub.status.idle": "2025-06-17T16:43:07.873256Z",
     "shell.execute_reply": "2025-06-17T16:43:07.872289Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.852711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "hydrophobicity_Kyte_Doolittle  = {\n",
    "    'A': 1.8,  'C': 2.5,  'D': -3.5, 'E': -3.5, 'F': 2.8,\n",
    "    'G': -0.4, 'H': -3.2, 'I': 4.5,  'K': -3.9, 'L': 3.8,\n",
    "    'M': 1.9,  'N': -3.5, 'P': -1.6, 'Q': -3.5, 'R': -4.5,\n",
    "    'S': -0.8, 'T': -0.7, 'V': 4.2,  'W': -0.9, 'Y': -1.3\n",
    "}\n",
    "\n",
    "Hydrophilicity_Hopp_Woods_scale  = {\n",
    "    'A': -0.5, 'C': -1.0, 'D': 3.0,  'E': 3.0,  'F': -2.5,\n",
    "    'G': 0.0,  'H': -0.5, 'I': -1.8, 'K': 3.0,  'L': -1.8,\n",
    "    'M': -1.3, 'N': 0.2,  'P': 0.0,  'Q': 0.2,  'R': 3.0,\n",
    "    'S': 0.3,  'T': -0.4, 'V': -1.5, 'W': -3.4, 'Y': -2.3\n",
    "}\n",
    "Polarity_Scale = {\n",
    "    'A': 8.1,  'C': 5.5,  'D': 13.0, 'E': 12.3, 'F': 5.2,\n",
    "    'G': 9.0,  'H': 10.4, 'I': 5.2,  'K': 11.3, 'L': 4.9,\n",
    "    'M': 5.7,  'N': 11.6, 'P': 8.0,  'Q': 10.5, 'R': 10.5,\n",
    "    'S': 9.2,  'T': 8.6,  'V': 5.9,  'W': 5.4,  'Y': 6.2\n",
    "}\n",
    "Molecular_Weight = {\n",
    "    'A': 89.09,  'C': 121.15, 'D': 133.10, 'E': 147.13, 'F': 165.19,\n",
    "    'G': 75.07,  'H': 155.16, 'I': 131.17, 'K': 146.19, 'L': 131.17,\n",
    "    'M': 149.21, 'N': 132.12, 'P': 115.13, 'Q': 146.15, 'R': 174.20,\n",
    "    'S': 105.09, 'T': 119.12, 'V': 117.15, 'W': 204.23, 'Y': 181.19\n",
    "}\n",
    "\n",
    "\n",
    "def geary_autocorrelation(sequence, max_lag=5, property_dict=hydrophobicity_Kyte_Doolittle):\n",
    "    \"\"\"\n",
    "    Feature #5: Geary Autocorrelation Descriptor\n",
    "\n",
    "    This function calculates the Geary autocorrelation descriptor for a given protein sequence,\n",
    "    which quantifies the spatial autocorrelation of a chosen physicochemical property across \n",
    "    different lags (distances) in the amino acid sequence.\n",
    "\n",
    "    Concept:\n",
    "    - For a selected amino acid property (e.g., hydrophobicity, polarity), this feature measures \n",
    "      how similar the property values are between amino acids separated by a lag `d`.\n",
    "    - It is based on the Geary's C metric (used in spatial statistics), which is sensitive to \n",
    "      local changes in the property values across the sequence.\n",
    "\n",
    "    Formula:\n",
    "    Geary(d) = (N - 1) * Σ[(Pᵢ - Pᵢ₊ₗ)²] / [2 * (N - d) * Σ(Pᵢ - mean(P))²]\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (str): Protein sequence using one-letter amino acid codes.\n",
    "    - max_lag (int): Maximum distance (lag) to consider for autocorrelation (default = 5).\n",
    "    - property_dict (dict): Dictionary mapping amino acids to a numeric property scale,\n",
    "                            such as hydrophobicity, hydrophilicity, polarity, or molecular weight.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of `max_lag` Geary autocorrelation values for lags from 1 to `max_lag`.\n",
    "    \"\"\"\n",
    "    prop_values = np.array([property_dict.get(aa, 0) for aa in sequence])  # Default 0 if AA is unknown\n",
    "    N = len(prop_values)\n",
    "    mean_p = np.mean(prop_values)\n",
    "\n",
    "    geary_values = {}\n",
    "    \n",
    "    for d in range(1, max_lag + 1):\n",
    "        numerator = np.sum((prop_values[:-d] - prop_values[d:]) ** 2)\n",
    "        denominator = 2 * (N - d) * np.sum((prop_values - mean_p) ** 2)\n",
    "        geary_values[f'Geary_Lag_{d}'] = (N - 1) * numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    return list(geary_values.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.874657Z",
     "iopub.status.busy": "2025-06-17T16:43:07.874357Z",
     "iopub.status.idle": "2025-06-17T16:43:07.899476Z",
     "shell.execute_reply": "2025-06-17T16:43:07.898315Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.874637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- Define sizes of each feature set ----\n",
    "vec_size = 30 + 15 + 343 + 400 + 5   \n",
    "def Features(S):\n",
    "    V = np.zeros((len(S), vec_size))\n",
    "    \n",
    "    for i, seq in enumerate(tqdm(S, desc=\"Processing sequences\")):\n",
    "        offset = 0\n",
    "\n",
    "        # APAAC: 30 features\n",
    "        V[i, offset:offset + 30] = apaac(seq)\n",
    "        offset += 30\n",
    "\n",
    "        # CTDD: 15 features\n",
    "        V[i, offset:offset + 15] = ctdd(seq)\n",
    "        offset += 15\n",
    "\n",
    "        # CTriad: 343 features\n",
    "        V[i, offset:offset + 343] = ctriad(seq)\n",
    "        offset += 343\n",
    "\n",
    "        # DDE: 400 features\n",
    "        V[i, offset:offset + 400] = dde(seq)\n",
    "        offset += 400\n",
    "\n",
    "        # Geary Autocorrelation: 5 features\n",
    "        V[i, offset:offset + 5] = geary_autocorrelation(seq)\n",
    "        offset += 5\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.906216Z",
     "iopub.status.busy": "2025-06-17T16:43:07.905701Z",
     "iopub.status.idle": "2025-06-17T16:43:07.916487Z",
     "shell.execute_reply": "2025-06-17T16:43:07.915477Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.906184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_feature_vector(df, algo=\"\"):\n",
    "    return algo(df['Sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T17:00:35.547347Z",
     "iopub.status.busy": "2025-06-17T17:00:35.547013Z",
     "iopub.status.idle": "2025-06-17T17:00:35.554570Z",
     "shell.execute_reply": "2025-06-17T17:00:35.553345Z",
     "shell.execute_reply.started": "2025-06-17T17:00:35.547324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cls_report(y_test,preds):\n",
    "    print(classification_report(y_test, preds, zero_division=0))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "\n",
    "def get_preds(model_name,X_test,y_test,Verbose=False):\n",
    "    preds = models[model_name].predict(X_test)\n",
    "    if Verbose:\n",
    "        print(f\"🎯 {model_name} Results:\")\n",
    "        cls_report(y_test,preds)\n",
    "    return preds\n",
    "\n",
    "    \n",
    "def con_matrix(y_test,preds):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    \n",
    "    #labels (0 = human, 1 = animal)\n",
    "    \n",
    "    # Create and display the confusion matrix with labels\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Human', 'Animal'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.931715Z",
     "iopub.status.busy": "2025-06-17T16:43:07.931386Z",
     "iopub.status.idle": "2025-06-17T16:43:07.953112Z",
     "shell.execute_reply": "2025-06-17T16:43:07.951748Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.931688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_extractors = {\n",
    "    \"Virus2VecModel\": Virus2Vec,\n",
    "    \"PwmVecModel\": Pwm2Vec,\n",
    "    \"FeaturesModel\": Features,  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.954753Z",
     "iopub.status.busy": "2025-06-17T16:43:07.954469Z",
     "iopub.status.idle": "2025-06-17T16:43:07.974144Z",
     "shell.execute_reply": "2025-06-17T16:43:07.972866Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.954728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test = np.array((df_test[\"Class\"].str.lower() != \"human\").astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.975470Z",
     "iopub.status.busy": "2025-06-17T16:43:07.975218Z",
     "iopub.status.idle": "2025-06-17T16:43:07.992739Z",
     "shell.execute_reply": "2025-06-17T16:43:07.991701Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.975446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models={}\n",
    "models[\"deepLearning\"] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:07.994275Z",
     "iopub.status.busy": "2025-06-17T16:43:07.993914Z",
     "iopub.status.idle": "2025-06-17T16:43:28.303495Z",
     "shell.execute_reply": "2025-06-17T16:43:28.302356Z",
     "shell.execute_reply.started": "2025-06-17T16:43:07.994253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the model of virus2vec using joblib\n",
    "Virus2VecModel = joblib.load('/kaggle/input/virogenml/scikitlearn/default/11/RF_Virus2vec.pkl')\n",
    "models[\"Virus2VecModel\"] = Virus2VecModel\n",
    "# get the feature vector due to virus2vec\n",
    "Virus2Vec_feature_vector = get_feature_vector(df_test,Virus2Vec)\n",
    "# get the preds of the model\n",
    "Virus2Vec_preds = get_preds('Virus2VecModel',Virus2Vec_feature_vector,y_test,Verbose=True)\n",
    "con_matrix(y_test,Virus2Vec_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:43:28.305082Z",
     "iopub.status.busy": "2025-06-17T16:43:28.304770Z",
     "iopub.status.idle": "2025-06-17T16:45:11.168130Z",
     "shell.execute_reply": "2025-06-17T16:45:11.167196Z",
     "shell.execute_reply.started": "2025-06-17T16:43:28.305059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the model of features using joblib\n",
    "FeaturesModel = joblib.load('/kaggle/input/virogenml/scikitlearn/default/11/SVM_ML_Features.pkl')\n",
    "models[\"FeaturesModel\"] = FeaturesModel\n",
    "# get the feature vector due to FeaturesModel\n",
    "FeaturesModel_feature_vector = get_feature_vector(df_test,Features)\n",
    "# get the preds of the FeaturesModel \n",
    "FeaturesModel_preds = get_preds('FeaturesModel',FeaturesModel_feature_vector,y_test,Verbose=True)\n",
    "con_matrix(y_test,FeaturesModel_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:45:11.169466Z",
     "iopub.status.busy": "2025-06-17T16:45:11.169168Z",
     "iopub.status.idle": "2025-06-17T16:45:32.610299Z",
     "shell.execute_reply": "2025-06-17T16:45:32.609360Z",
     "shell.execute_reply.started": "2025-06-17T16:45:11.169443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the model of Pwm2Vec using joblib\n",
    "PwmVecModel = joblib.load('/kaggle/input/virogenml/scikitlearn/default/11/SVM_PWM2Vec.pkl')\n",
    "models[\"PwmVecModel\"] = PwmVecModel\n",
    "# get the feature vector due to Pwm2Vec\n",
    "Pwm2Vec_feature_vector = get_feature_vector(df_test,Pwm2Vec)\n",
    "# get the preds of the Pwm2Vec model\n",
    "Pwm2Vec_preds = get_preds('PwmVecModel',Pwm2Vec_feature_vector,y_test,Verbose=True)\n",
    "con_matrix(y_test,Pwm2Vec_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:45:32.611540Z",
     "iopub.status.busy": "2025-06-17T16:45:32.611279Z",
     "iopub.status.idle": "2025-06-17T16:45:32.618944Z",
     "shell.execute_reply": "2025-06-17T16:45:32.617950Z",
     "shell.execute_reply.started": "2025-06-17T16:45:32.611520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(datas,ids,seq_ids,labels,ft_model):\n",
    "    datas = [ASW(sequence,l_sub) for sequence in datas.tolist()]\n",
    "    labels= np.repeat(labels, N).tolist()\n",
    "    ids=np.repeat(ids, N).tolist()\n",
    "    seq_ids=np.repeat(seq_ids, N).tolist()\n",
    "    \n",
    "    # Apply FastText (CBOW)\n",
    "    keys_wv=set(list(ft_model.wv.key_to_index.keys()))\n",
    "    \n",
    "    embeddings = np.array([\n",
    "        ft_model.wv[k]  # FastText will handle unknown k-mers\n",
    "        for kmer in tqdm(datas, desc=\"FastText inference\")\n",
    "        for k in kmer\n",
    "    ])\n",
    "    embeddings=torch.tensor(embeddings).to(device)\n",
    "    ids=torch.tensor(ids).to(device)\n",
    "    seq_ids=torch.tensor(seq_ids).to(device)\n",
    "    labels=torch.tensor(labels).to(device)\n",
    "    return embeddings,ids,seq_ids,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:45:32.620103Z",
     "iopub.status.busy": "2025-06-17T16:45:32.619875Z",
     "iopub.status.idle": "2025-06-17T16:45:32.647380Z",
     "shell.execute_reply": "2025-06-17T16:45:32.646346Z",
     "shell.execute_reply.started": "2025-06-17T16:45:32.620082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_deep_ids=df_test[\"Virus_ID\"]\n",
    "test_deep_seq_ids=df_test[\"Seq_ID\"]+\" \"+df_test[\"Virus_ID\"]\n",
    "\n",
    "# convert test string id to numeric\n",
    "_,test_deep_ids = np.unique(test_deep_ids, return_inverse=True)\n",
    "_,test_deep_seq_ids = np.unique(test_deep_seq_ids, return_inverse=True)\n",
    "test_deep_datas = df_test['Sequence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:45:32.648594Z",
     "iopub.status.busy": "2025-06-17T16:45:32.648343Z",
     "iopub.status.idle": "2025-06-17T16:47:18.277418Z",
     "shell.execute_reply": "2025-06-17T16:47:18.276483Z",
     "shell.execute_reply.started": "2025-06-17T16:45:32.648574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = create_data_loader(test_deep_datas, y_test,test_deep_ids,test_deep_seq_ids,ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:47:18.278903Z",
     "iopub.status.busy": "2025-06-17T16:47:18.278563Z",
     "iopub.status.idle": "2025-06-17T16:51:35.057225Z",
     "shell.execute_reply": "2025-06-17T16:51:35.056270Z",
     "shell.execute_reply.started": "2025-06-17T16:47:18.278880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred_deep,true_deep=test_deep(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:51:35.058501Z",
     "iopub.status.busy": "2025-06-17T16:51:35.058236Z",
     "iopub.status.idle": "2025-06-17T16:51:35.067850Z",
     "shell.execute_reply": "2025-06-17T16:51:35.066856Z",
     "shell.execute_reply.started": "2025-06-17T16:51:35.058482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert to numpy array for vectorized comparison\n",
    "test_deep_ids = np.array(test_deep_ids)\n",
    "unique_ids = np.unique(test_deep_ids)\n",
    "\n",
    "# Create mapping: sequence ID → corresponding prediction\n",
    "seq_pred_map = {seq_id: pred for seq_id, pred in zip(unique_ids, pred_deep)}\n",
    "\n",
    "# Generate output array by mapping each test_deep_id to its prediction\n",
    "seq_level_preds = np.array([seq_pred_map[seq_id] for seq_id in test_deep_ids])\n",
    "print(len(seq_level_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:51:35.069175Z",
     "iopub.status.busy": "2025-06-17T16:51:35.068846Z",
     "iopub.status.idle": "2025-06-17T16:51:35.084314Z",
     "shell.execute_reply": "2025-06-17T16:51:35.083306Z",
     "shell.execute_reply.started": "2025-06-17T16:51:35.069135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_models_roc_curves(models: dict, model_inputs: dict, y_test):\n",
    "    \"\"\"\n",
    "    Plots ROC curves for multiple classifiers using the same color palette as the precision-recall plot.\n",
    "\n",
    "    Parameters:\n",
    "    - models (dict): Keys are model names, values are fitted model instances.\n",
    "    - model_inputs (dict): Keys are model names, values are corresponding X_test feature vectors.\n",
    "    - y_test: True labels (0 = Human, 1 = Animal). Class 0 is treated as the positive class.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    distinct_colors = sns.color_palette(\"hls\", len(models))  # Consistent colors\n",
    "\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        X = model_inputs[name]\n",
    "        # Predict probabilities\n",
    "        try:\n",
    "            y_score = model.predict_proba(X)[:, 0]  # Probability of class 0 (Human)\n",
    "        except AttributeError:\n",
    "            y_score=seq_level_preds\n",
    "            y_score=1-y_score\n",
    "            \n",
    "\n",
    "        # Compute ROC metricspred_l\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=0)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, lw=2, color=distinct_colors[i],\n",
    "                 label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Plot random guess line\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves (Human = Positive Class)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:59:16.677002Z",
     "iopub.status.busy": "2025-06-17T16:59:16.676627Z",
     "iopub.status.idle": "2025-06-17T16:59:16.686473Z",
     "shell.execute_reply": "2025-06-17T16:59:16.685498Z",
     "shell.execute_reply.started": "2025-06-17T16:59:16.676976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall_max_precision_at_full_recall(models, model_inputs, y_test):\n",
    "    \"\"\"\n",
    "    Plots precision vs recall curves for multiple models (with different feature vectors),\n",
    "    and highlights the max precision at recall = 1.0 point for each model.\n",
    "\n",
    "    Parameters:\n",
    "    - models: dict of model_name → trained model\n",
    "    - model_inputs: dict of model_name → corresponding X_test\n",
    "    - y_test: true labels (same for all models)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    distinct_colors = sns.color_palette(\"hls\", len(models))  # distinct colors per model\n",
    "\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        X = model_inputs[name]\n",
    "      \n",
    "        # Get prediction scores\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_scores = model.predict_proba(X)[:, 0]\n",
    "        else:\n",
    "            y_scores=seq_level_preds\n",
    "            y_scores=1-y_scores\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_scores, pos_label=0)\n",
    "\n",
    "\n",
    "        # Get max precision at recall = 1.0\n",
    "        recall_1_indices = np.where(recall == 1.0)[0]\n",
    "        if len(recall_1_indices) > 0:\n",
    "            max_prec_at_recall_1 = np.max(precision[recall_1_indices])\n",
    "            plt.scatter(max_prec_at_recall_1, 1.0, color=distinct_colors[i], marker='o',\n",
    "                        edgecolor='black', s=100,\n",
    "                        label=f'{name} Max Prec @ R=1.0: {max_prec_at_recall_1:.2f}')\n",
    "        else:\n",
    "            print(f'{name} never reaches 100% recall.')\n",
    "\n",
    "        # Plot Precision (X) vs Recall (Y)\n",
    "        plt.plot(precision, recall, label=name, color=distinct_colors[i])\n",
    "\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Precision vs Recall Curves (Highlight Max Precision @ Recall=1.0)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:51:35.099972Z",
     "iopub.status.busy": "2025-06-17T16:51:35.099723Z",
     "iopub.status.idle": "2025-06-17T16:51:35.120964Z",
     "shell.execute_reply": "2025-06-17T16:51:35.120029Z",
     "shell.execute_reply.started": "2025-06-17T16:51:35.099954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    \"Virus2VecModel\": Virus2Vec_feature_vector,\n",
    "    \"FeaturesModel\": FeaturesModel_feature_vector,\n",
    "    \"PwmVecModel\": Pwm2Vec_feature_vector,\n",
    "    \"deepLearning\":None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:51:35.122124Z",
     "iopub.status.busy": "2025-06-17T16:51:35.121788Z",
     "iopub.status.idle": "2025-06-17T16:52:57.601761Z",
     "shell.execute_reply": "2025-06-17T16:52:57.600856Z",
     "shell.execute_reply.started": "2025-06-17T16:51:35.122094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_precision_vs_recall_max_precision_at_full_recall(models, model_inputs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:52:57.602923Z",
     "iopub.status.busy": "2025-06-17T16:52:57.602631Z",
     "iopub.status.idle": "2025-06-17T16:54:19.966932Z",
     "shell.execute_reply": "2025-06-17T16:54:19.965764Z",
     "shell.execute_reply.started": "2025-06-17T16:52:57.602903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_models_roc_curves(models, model_inputs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T16:54:19.968309Z",
     "iopub.status.busy": "2025-06-17T16:54:19.968035Z",
     "iopub.status.idle": "2025-06-17T16:54:19.977546Z",
     "shell.execute_reply": "2025-06-17T16:54:19.976543Z",
     "shell.execute_reply.started": "2025-06-17T16:54:19.968288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_deep(df,y_true):\n",
    "    test_deep_ids=df[\"Virus_ID\"]\n",
    "    test_deep_seq_ids=df[\"Seq_ID\"]+\" \"+df[\"Virus_ID\"]\n",
    "    \n",
    "    # convert test string id to numeric\n",
    "    _,test_deep_ids = np.unique(test_deep_ids, return_inverse=True)\n",
    "    _,test_deep_seq_ids = np.unique(test_deep_seq_ids, return_inverse=True)\n",
    "    test_deep_datas = df['Sequence']\n",
    "\n",
    "    DL = create_data_loader(test_deep_datas, y_true,test_deep_ids,test_deep_seq_ids,ft_model)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    output = []\n",
    "    true_lables=[]\n",
    "    with torch.no_grad():\n",
    "         for batch_data, batch_ids,batch_seq_ids, batch_labels in tqdm(  DL, desc=\"Processing Batches\"):\n",
    "            batch_data,batch_ids,batch_seq_ids, batch_labels  = batch_data.to(device), batch_ids.to(device),batch_seq_ids.to(device), batch_labels.to(device)\n",
    "            Y_prob, Y_hat, A, A_2 =model(batch_data,batch_ids,batch_seq_ids)         \n",
    "            output += Y_hat.cpu().tolist()\n",
    "            true_lables += batch_labels.cpu().tolist()\n",
    "            # Y_prob=Y_prob.squeeze(1)\n",
    "            # probs += Y_prob.squeeze(1).cpu().tolist() \n",
    "            Y_hat = Y_hat.view_as(batch_labels)\n",
    "\n",
    "    \n",
    "        # Convert to numpy array for vectorized comparison\n",
    "    test_deep_ids = np.array(test_deep_ids)\n",
    "    unique_ids = np.unique(test_deep_ids)\n",
    "        \n",
    "        # Create mapping: sequence ID → corresponding prediction\n",
    "    seq_pred_map = {seq_id: pred for seq_id, pred in zip(unique_ids, output)}\n",
    "        \n",
    "        # Generate output array by mapping each test_deep_id to its prediction\n",
    "    seq_level_preds = np.array([seq_pred_map[seq_id] for seq_id in test_deep_ids])\n",
    "    return seq_level_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T17:02:25.763412Z",
     "iopub.status.busy": "2025-06-17T17:02:25.763076Z",
     "iopub.status.idle": "2025-06-17T17:02:25.774439Z",
     "shell.execute_reply": "2025-06-17T17:02:25.773531Z",
     "shell.execute_reply.started": "2025-06-17T17:02:25.763388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_models_on_pandemics(pandemic_dfs, model_ml_name, alphabet=alphabet, k=k, m=m):\n",
    "    for flu_name, df in pandemic_dfs.items():\n",
    "        feature_fn = feature_extractors[model_ml_name]\n",
    "        feature_vector = get_feature_vector(df, feature_fn)\n",
    "        y_true = (df[\"Class\"].str.lower() != \"human\").astype(int).values\n",
    "        # Machine Learning predictions\n",
    "        y_pred_ml = get_preds(model_ml_name, feature_vector, y_true)\n",
    "        # Deep Learning predictions\n",
    "        y_pred_dl = predict_deep(df, y_true)\n",
    "\n",
    "        # Compute confusion matrices\n",
    "        cm_ml = confusion_matrix(y_true, y_pred_ml, labels=[0, 1])\n",
    "        cm_dl = confusion_matrix(y_true, y_pred_dl, labels=[0, 1])\n",
    "\n",
    "        # Plot heatmap confusion matrices\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        fig.suptitle(f'{flu_name} - Confusion Matrices', fontsize=16)\n",
    "\n",
    "        sns.heatmap(cm_ml, annot=True, fmt='d', cmap='Blues', ax=axs[0],\n",
    "            xticklabels=[\"Human\", \"Animal\"], yticklabels=[\"Human\", \"Animal\"])\n",
    "        axs[0].set_title('Machine Learning')\n",
    "        axs[0].set_xlabel('Predicted')\n",
    "        axs[0].set_ylabel('True')\n",
    "\n",
    "        sns.heatmap(cm_dl, annot=True, fmt='d', cmap='Greens', ax=axs[1],\n",
    "            xticklabels=[\"Human\", \"Animal\"], yticklabels=[\"Human\", \"Animal\"])\n",
    "\n",
    "        axs[1].set_title('Deep Learning')\n",
    "        axs[1].set_xlabel('Predicted')\n",
    "        axs[1].set_ylabel('True')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T17:02:27.730578Z",
     "iopub.status.busy": "2025-06-17T17:02:27.730270Z",
     "iopub.status.idle": "2025-06-17T17:02:58.587236Z",
     "shell.execute_reply": "2025-06-17T17:02:58.586008Z",
     "shell.execute_reply.started": "2025-06-17T17:02:27.730557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pandemics = {\n",
    "    \"Asian Flu\": df_Asian_flu,\n",
    "    \"Hong Kong Flu\": df_hong_kong_flu,\n",
    "    \"Swine Flu\": df_pdmh1n1_flu,\n",
    "    \"covid Flu\": df_covid,\n",
    "}\n",
    "\n",
    "evaluate_models_on_pandemics(pandemics, model_ml_name=\"Virus2VecModel\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6981441,
     "sourceId": 12123416,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7663838,
     "sourceId": 12168419,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 370718,
     "modelInstanceId": 349458,
     "sourceId": 428707,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 373006,
     "modelInstanceId": 351745,
     "sourceId": 438204,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
