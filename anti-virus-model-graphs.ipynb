{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12034111,"sourceType":"datasetVersion","datasetId":7572023},{"sourceId":12046935,"sourceType":"datasetVersion","datasetId":7581212}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install biopython\n!pip install --upgrade --no-cache-dir biopython\n!pip install rdkit-pypi\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu118.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.2.0+cu118.html\n!pip install -q torch-geometric\n!pip install fair-esm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:21.573018Z","iopub.execute_input":"2025-06-03T18:25:21.573210Z","iopub.status.idle":"2025-06-03T18:25:43.119665Z","shell.execute_reply.started":"2025-06-03T18:25:21.573190Z","shell.execute_reply":"2025-06-03T18:25:43.113960Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.data import Batch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\nimport pickle\nfrom torch.utils.data import DataLoader, Subset, random_split\nimport esm\nfrom joblib import Parallel, delayed\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:45.087230Z","iopub.execute_input":"2025-06-03T18:25:45.087528Z","iopub.status.idle":"2025-06-03T18:25:50.001754Z","shell.execute_reply.started":"2025-06-03T18:25:45.087503Z","shell.execute_reply":"2025-06-03T18:25:49.997387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pIC50=np.load('/kaggle/input/drug-virus-features/pIC50.npy')\nwith open(\"/kaggle/input/drug-virus-features/drug_graphs.pkl\", \"rb\") as f:\n    drug_graphs = pickle.load(f)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:50.002663Z","iopub.execute_input":"2025-06-03T18:25:50.003092Z","iopub.status.idle":"2025-06-03T18:25:54.137963Z","shell.execute_reply.started":"2025-06-03T18:25:50.003064Z","shell.execute_reply":"2025-06-03T18:25:54.132078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef esm_model(model,alphabet,seq):\n   \n    batch_converter = alphabet.get_batch_converter()\n    batch_labels, batch_strs, batch_tokens = batch_converter([(\"protein\", seq)])\n    with torch.no_grad():\n        results = model(batch_tokens, repr_layers=[16], return_contacts=True)\n    \n    contact_map = results[\"contacts\"]  # Shape: [1, L, L]\n    \n   \n    return contact_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:54.138876Z","iopub.execute_input":"2025-06-03T18:25:54.139176Z","iopub.status.idle":"2025-06-03T18:25:54.150010Z","shell.execute_reply.started":"2025-06-03T18:25:54.139152Z","shell.execute_reply":"2025-06-03T18:25:54.146423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_sequence(seq, window_size=1000, stride=500):\n    windows = []\n    for start in range(0, len(seq), stride):\n        end = min(start + window_size, len(seq))\n        if end - start < 2:  # skip too-short fragments\n            break\n        windows.append((start, seq[start:end]))\n        if end == len(seq):\n            break\n    return windows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:54.153672Z","iopub.execute_input":"2025-06-03T18:25:54.153951Z","iopub.status.idle":"2025-06-03T18:25:54.167271Z","shell.execute_reply.started":"2025-06-03T18:25:54.153930Z","shell.execute_reply":"2025-06-03T18:25:54.161761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def protein_graph(model, alphabet, seq, threshold=0.5, window_size=1000, stride=500):\n    aa_dict = {aa: i for i, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n    L = len(seq)\n    \n    # Build node features (one-hot encoding for the full sequence)\n    node_features = torch.eye(20)[[aa_dict.get(aa, 0) for aa in seq]]  # [L, 20]\n\n    # Containers for merged edges\n    edge_index = []\n    edge_attr = []\n\n    windows = split_sequence(seq, window_size, stride)\n\n    for start_idx, subseq in windows:\n        contact_map = esm_model(model, alphabet, subseq)[0]  # shape: [L_window, L_window]\n        L_win = len(subseq)\n\n        for i in range(L_win):\n            for j in range(L_win):\n                prob = contact_map[i, j].item()\n                if prob > threshold:\n                    global_i = start_idx + i\n                    global_j = start_idx + j\n                    if global_i < L and global_j < L:\n                        edge_index.append([global_i, global_j])\n                        edge_attr.append(prob)\n\n    return node_features, edge_index, edge_attr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:54.168189Z","iopub.execute_input":"2025-06-03T18:25:54.168434Z","iopub.status.idle":"2025-06-03T18:25:54.182650Z","shell.execute_reply.started":"2025-06-03T18:25:54.168411Z","shell.execute_reply":"2025-06-03T18:25:54.177964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the CSV file\ndf = pd.read_csv(\"/kaggle/input/virus-drug/virus_drug_interactions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:54.185213Z","iopub.execute_input":"2025-06-03T18:25:54.185523Z","iopub.status.idle":"2025-06-03T18:25:54.321639Z","shell.execute_reply.started":"2025-06-03T18:25:54.185499Z","shell.execute_reply":"2025-06-03T18:25:54.316725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\nmodel.eval()\nprotein_sequences = df['Protein_Sequence']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T18:25:54.629019Z","iopub.execute_input":"2025-06-03T18:25:54.629390Z","execution_failed":"2025-06-03T18:28:55.331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef compute_graph(protein):\n   \n    return protein_graph(model, alphabet, protein)\n\n# Run in parallel using all CPU cores\nprotein_graphs = Parallel(n_jobs=4)(\n    delayed(compute_graph)(protein) for protein in tqdm(protein_sequences, desc=\"Processing proteins\")\n)\n\n# Print results\nprint(len(protein_graphs), len(protein_graphs[0]))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-03T18:28:55.332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_size = len(protein_features)\ntrain_size = int(0.7 * total_size)\nval_size = int(0.15 * total_size)\ntest_size = total_size - train_size - val_size\n\nall_indices = list(range(total_size))\ntrain_indices, val_indices, test_indices = random_split(\n    all_indices, [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_protein = protein_features[train_indices] \npca = PCA(n_components=50)\npca.fit(train_protein)\n\n# Step 3: Transform all sets\nprotein_pca = pca.transform(protein_features)\nprotein_pca = torch.tensor(protein_pca, dtype=torch.float32)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def drug_graph_to_data(drug_graph):\n    mol_size, nodes, edges, edges_type = drug_graph\n    x = torch.tensor(nodes, dtype=torch.float)  # [num_nodes, node_features]\n    \n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # [2, num_edges]\n    edge_attr = torch.tensor(edges_type, dtype=torch.float).unsqueeze(1)  # [num_edges, 1]\n    \n    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def protein_graph_to_data(protein_graph):\n    node_features,edge_index,edge_attr = protein_graph\n    x = node_features\n    \n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # [2, num_edges]\n    edge_attr = torch.tensor(edge_attr, dtype=torch.float).unsqueeze(1)  # [num_edges, 1]\n    \n    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DrugProteinDataset(torch.utils.data.Dataset):\n    def __init__(self, protein_features, drug_graphs, pIC50_values):\n        self.protein_features = protein_features\n        self.drug_graphs = drug_graphs\n        self.pIC50_values = pIC50_values\n    \n    def __len__(self):\n        return len(self.pIC50_values)\n    \n    def __getitem__(self, idx):\n        protein_feature = torch.tensor(self.protein_features[idx], dtype=torch.float)\n        drug_graph = drug_graph_to_data(self.drug_graphs[idx])\n        pIC50_value = torch.tensor(self.pIC50_values[idx], dtype=torch.float)\n        return protein_feature, drug_graph, pIC50_value\n\ndef custom_collate(batch):\n    protein_feats = torch.stack([item[0] for item in batch])  # [batch_size, protein_feature_dim]\n    drug_graphs = [item[1] for item in batch]                 # List of PyG Data objects\n    labels = torch.stack([item[2] for item in batch])         # [batch_size]\n\n    batch_drug_graphs = Batch.from_data_list(drug_graphs)     # Combine graphs into a single batched graph\n\n    return protein_feats, batch_drug_graphs, labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DrugTargetGNN(nn.Module):\n    def __init__(self, node_feature_dim=78, protein_feature_dim=50, hidden_dim=128):\n        super().__init__()\n        # GNN layers for drug graph\n        self.conv1 = GCNConv(node_feature_dim, node_feature_dim)\n        self.conv2 = GCNConv(node_feature_dim, node_feature_dim*2)\n        self.conv3 = GCNConv(node_feature_dim*2, node_feature_dim*4)\n\n        \n        self.lineargraph = nn.Linear(node_feature_dim*4, hidden_dim)\n        \n        # MLP for protein features\n        self.protein_mlp = nn.Sequential(\n            nn.Linear(protein_feature_dim, protein_feature_dim*2),\n            nn.ReLU(),\n            nn.Linear(protein_feature_dim*2, protein_feature_dim*4),\n            nn.ReLU(),\n            nn.Linear(protein_feature_dim*4, hidden_dim)\n            \n        )\n        \n        # Final layers for combined features\n        self.final_mlp = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            \n            nn.Linear(hidden_dim, hidden_dim//2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim//2, 1)  # regression output for pIC50\n        )\n\n        \n    def forward(self, protein_feat, drug_graph):\n        # GNN on drug graph\n        x, edge_index,edge_attr = drug_graph.x, drug_graph.edge_index,drug_graph.edge_attr\n        x = F.relu(self.conv1(x, edge_index,edge_attr))\n        x = F.relu(self.conv2(x, edge_index,edge_attr))\n        x = F.relu(self.conv3(x, edge_index,edge_attr))\n        \n        x = global_mean_pool(x, drug_graph.batch)  # [batch_size, hidden_dim]\n        x = F.relu(self.lineargraph(x))\n\n        \n        # Protein feature embedding\n        p = self.protein_mlp(protein_feat)  # [batch_size, hidden_dim]\n        \n        # Combine embeddings\n        combined = torch.cat([x, p], dim=1)\n        out = self.final_mlp(combined)\n        return out.squeeze()  # [batch_size]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndataset = DrugProteinDataset(protein_pca, drug_graphs, pIC50)\n\ntrain_dataset = Subset(dataset, train_indices)\nval_dataset = Subset(dataset, val_indices)\ntest_dataset = Subset(dataset, test_indices)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=custom_collate)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset = DrugProteinDataset(protein_features, drug_graphs, pIC50)\n\n# # Step 1: Load the original CSV and get top 10 virus indices\n# df = pd.read_csv('/kaggle/input/virus-drug/virus_drug_interactions.csv')\n# top_3_viruses = df['Virus_Organism'].value_counts().head(3).index\n# top_3_indices = df[df['Virus_Organism'].isin(top_3_viruses)].index.tolist()\n\n# # Step 2: Get remaining indices\n# all_indices = set(range(len(df)))\n# remaining_indices = list(all_indices - set(top_3_indices))\n\n# # Step 3: Split remaining into val and test (50/50)\n# remaining_size = len(remaining_indices)\n# val_size = remaining_size // 2\n# test_size = remaining_size - val_size  # ensures no rounding errors\n\n# val_indices, test_indices = random_split(\n#     remaining_indices,\n#     [val_size, test_size],\n#     generator=torch.Generator().manual_seed(42)\n# )\n\n# # Step 4: Build subsets\n# train_dataset = Subset(dataset, top_10_indices)\n# val_dataset = Subset(dataset, val_indices)\n# test_dataset = Subset(dataset, test_indices)\n\n# # Step 5: Build DataLoaders\n# train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=custom_collate)\n# val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate)\n# test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DrugTargetGNN().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()  # for regression\n\nmodel.train()\nfor epoch in range(20):\n    total_loss = 0\n    for protein_feat, drug_graph, values in train_loader:\n        protein_feat = protein_feat.to(device)\n        drug_graph = drug_graph.to(device)\n        values = values.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(protein_feat, drug_graph)\n        loss = criterion(outputs, values)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef evaluate(model, dataloader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for protein_feats, drug_graphs, values in dataloader:\n            protein_feats = protein_feats.to(device)\n            drug_graphs = drug_graphs.to(device)\n            values = values.to(device)\n\n            outputs = model(protein_feats, drug_graphs)\n            all_preds.append(outputs.cpu())\n            all_labels.append(values.cpu())\n         \n    preds = torch.cat(all_preds).numpy()\n   \n    values = torch.cat(all_labels).numpy()\n\n    mse = mean_squared_error(values, preds)\n    rmse = mse ** 0.5\n    pearson_corr, _ = pearsonr(values, preds)\n\n    return {\n        \"MSE\": mse,\n        \"RMSE\": rmse,\n        \"Pearson\": pearson_corr\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nval_metrics = evaluate(model, val_loader, device)\nprint(\"Validation Metrics:\", val_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/virus-drug/virus_drug_interactions.csv')\n\n# Count the number of occurrences for each unique Virus_Organism\nvirus_counts = df['Virus_Organism'].value_counts()\n\n# Get the top 10 most frequent Virus_Organisms\ntop_10_viruses = virus_counts.head(3).index\n\n# Get the indices of these top 10 viruses in the original DataFrame\ntop_10_indices = df[df['Virus_Organism'].isin(top_10_viruses)].index.tolist()\n\n# Print the indices\nprint(\"Indices of top 10 Virus_Organisms in the original file:\")\nprint(len(top_10_indices))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(top_3_indices))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}