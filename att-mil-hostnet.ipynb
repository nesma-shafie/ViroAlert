{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10912546,"sourceType":"datasetVersion","datasetId":6783443},{"sourceId":11035092,"sourceType":"datasetVersion","datasetId":6872786},{"sourceId":11052529,"sourceType":"datasetVersion","datasetId":6878768},{"sourceId":11197186,"sourceType":"datasetVersion","datasetId":6981441}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install --no-cache-dir gensim\n# import gensim\n# print(gensim.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:05.867867Z","iopub.execute_input":"2025-03-29T12:30:05.868269Z","iopub.status.idle":"2025-03-29T12:30:05.872755Z","shell.execute_reply.started":"2025-03-29T12:30:05.868240Z","shell.execute_reply":"2025-03-29T12:30:05.871508Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom __future__ import print_function\nimport argparse\nimport numpy as np\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\nfrom collections import Counter\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec,KeyedVectors\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nimport os\nimport pandas as pd\nfrom itertools import chain\ntorch.manual_seed(42)\nnp.random.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Auto-detect GPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:05.886447Z","iopub.execute_input":"2025-03-29T12:30:05.886805Z","iopub.status.idle":"2025-03-29T12:30:28.593556Z","shell.execute_reply.started":"2025-03-29T12:30:05.886781Z","shell.execute_reply":"2025-03-29T12:30:28.592846Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def readDataFromFile(filename):\n    file_path = os.path.abspath(filename)  # Ensure absolute path\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n\n    # Rename columns for clarity\n    df.columns = [\"ID\", \"Sequence\"]\n\n    # Extract virus_ID (part after first \"|\") and seq_ID (part after second \"|\")\n    df[\"Virus_ID\"] = df[\"ID\"].apply(lambda x: \"\".join(x.split(\"|\")[1:]) if \"|\" in x else \"\")\n    df[\"Seq_ID\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[0] if \"|\" in x else \"\")\n    df[\"Class\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[-1] if \"|\" in x else \"\")\n    df[\"Length\"] = df[\"Sequence\"].apply(lambda x: len(x))\n\n    return df[[\"Sequence\",\"Virus_ID\", \"Seq_ID\", \"Class\",\"Length\"]]  # Return relevant columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.594653Z","iopub.execute_input":"2025-03-29T12:30:28.595190Z","iopub.status.idle":"2025-03-29T12:30:28.600630Z","shell.execute_reply.started":"2025-03-29T12:30:28.595165Z","shell.execute_reply":"2025-03-29T12:30:28.599828Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def ASW(sequence, l_sub, n):\n    \"\"\"\n        sequence (str): The original viral sequence.\n        l_sub (int): The length of each subsequence.\n        n (int): The number of subsequences to generate.\n    \"\"\"\n    l = len(sequence)\n    \n \n    if n > 1:\n        l_stride = (l - l_sub) // (n - 1)\n    else:\n        l_stride = 1  \n    \n    subsequences = []\n \n    for i in range(0, min(n * l_stride, l - l_sub + 1), l_stride):\n        subsequences.append(sequence[i:i + l_sub])\n    \n    return subsequences\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.602370Z","iopub.execute_input":"2025-03-29T12:30:28.602709Z","iopub.status.idle":"2025-03-29T12:30:28.629102Z","shell.execute_reply.started":"2025-03-29T12:30:28.602679Z","shell.execute_reply":"2025-03-29T12:30:28.628473Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 0->human  1-> animals\nclass GatedAttention(nn.Module):\n    def __init__(self,nhead,encoderNlayers,embeddingSize,intermidiateDim):\n        super(GatedAttention, self).__init__()\n        self.M = embeddingSize\n        self.L = intermidiateDim\n        self.encoderNlayers=encoderNlayers\n        self.ATTENTION_BRANCHES = 1\n        self.nhead=nhead\n\n        # embedding \n        self.encoder_layer = TransformerEncoderLayer(d_model=self.M, nhead=self.nhead)\n        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=self.encoderNlayers)\n        \n        # instance level \n        self.attention_V_1 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix V\n            nn.Tanh()\n        )\n\n        self.attention_U_1 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix U\n            nn.Sigmoid()\n        )\n\n        self.attention_w_1 = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n\n\n        # bag level \n        self.attention_V_2 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix V\n            nn.Tanh()\n        )\n\n        self.attention_U_2 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix U\n            nn.Sigmoid()\n        )\n\n        self.attention_w_2 = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n\n\n        \n        # classifier\n        self.classifier = nn.Sequential(\n            nn.Conv1d(in_channels=self.ATTENTION_BRANCHES, out_channels=128, kernel_size=4, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2),\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=7, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2),\n            nn.Flatten(),  # Converts to 1D before fully connected layers\n            nn.Linear(128 * ((self.M) // 4), 256),  # Adjust size based on sequence length\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 1),\n            nn.Sigmoid() \n        )\n\n    \n    def forward(self, datas,ids,Seq_ids):\n        #### STEP 1:embeddings\n        datas = datas.float()  # Ensure correct dtype\n        instances=self.transformer_encoder(datas) \n        \n        #### STEP 2: INSTANCE-LEVEL ATTENTION ####\n        # Apply attention mechanisms per bag (over instances_per_bag)\n        A_V = self.attention_V_1(instances)  \n        A_U = self.attention_U_1(instances)  \n        A = self.attention_w_1(A_V * A_U)\n        A = torch.transpose(A, 1, 0)  \n        inner_bags = torch.unique_consecutive(Seq_ids)\n      \n        output = torch.empty(((len(inner_bags), self.M))).to(device)\n        super_ids = torch.empty(((len(inner_bags))))\n        for i, bag in enumerate(inner_bags):\n            A_vec=F.softmax(A[0][Seq_ids == bag],dim=0)\n            output[i] = torch.matmul(A_vec, instances[Seq_ids == bag])\n            super_ids[i]=ids[Seq_ids == bag][0]\n        \n        ### STEP 3: BAG-LEVEL ATTENTION ####\n        A_V_2 = self.attention_V_2(output)  \n        A_U_2 = self.attention_U_2(output)  \n        A_2 = self.attention_w_2(A_V_2 * A_U_2)  \n        A_2 = torch.transpose(A_2, 1,0)   \n\n      \n        outer_bags = torch.unique_consecutive(super_ids)\n        output2 = torch.empty(((len(outer_bags), self.M))).to(device)\n\n        for i, bag in enumerate(outer_bags):\n            A_vec_2=F.softmax(A_2[0][super_ids == bag],dim=0)\n            output2[i] = torch.matmul(A_vec_2, output[super_ids == bag])\n\n        \n        \n        ### STEP 4: CLASSIFICATION ####\n        # output2 = output2.view(output2.shape[0], -1)  # Flatten over bags_per_bag for classification\n        output2 = output2.unsqueeze(1)  # Add a channel dimension\n\n\n        Y_prob = self.classifier(output2)  # Shape: [batch_size, 1]\n        Y_hat = torch.ge(Y_prob, 0.5).float()  # Convert probabilities to binary predictions\n        return Y_prob, Y_hat, A","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.630285Z","iopub.execute_input":"2025-03-29T12:30:28.630545Z","iopub.status.idle":"2025-03-29T12:30:28.651047Z","shell.execute_reply.started":"2025-03-29T12:30:28.630526Z","shell.execute_reply":"2025-03-29T12:30:28.650445Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MILDataset(Dataset):\n    def __init__(self, datas, ids, seq_ids, labels):\n        self.datas = datas  # Instance features\n        self.ids = ids # Virus (outer bag) IDs\n        self.seq_ids = seq_ids  # Sequence (inner bag) IDs\n        self.labels = labels.to(\"cpu\")  # Labels at the virus (outer bag) level\n\n        # Unique IDs for outer bags (viruses) and their indices\n        self.unique_virus_ids, self.virus_indices = torch.unique(self.ids, return_inverse=True)\n        \n        # Unique IDs for inner bags (sequences) and their indices\n        self.unique_seq_ids, self.seq_indices = torch.unique(self.seq_ids, return_inverse=True)\n\n        # Mapping from virus to instance indices  2d array each list is the virus data indecies\n        self.virus_bag_indices_list = [torch.where(self.virus_indices == i)[0].to(\"cpu\") for i in tqdm(range(len(self.unique_virus_ids)))]\n\n        # Mapping from sequence to instance indices 2d array each list is the seq data indecies\n        self.seq_bag_indices_list = [torch.where(self.seq_indices == i)[0].to(\"cpu\") for i in tqdm( range(len(self.unique_seq_ids)))]\n\n        # Labels assigned at the virus level (each virus gets one label)\n        self.virus_labels = [self.labels[indices[0]] for indices in self.virus_bag_indices_list]\n\n        # Precomputed bag-of-bags structure (virus → [seq])\n        self.virus_seq_map = {}  # Maps virus_id -> list of sequence indices\n        for i, virus_id in tqdm(enumerate(self.unique_virus_ids)):\n            self.virus_seq_map[virus_id.item()] = list((self.seq_ids[self.virus_bag_indices_list[i]].tolist()))\n\n        # Precomputed bag IDs for each virus and sequence\n        self.precomputed_virus_ids = [torch.full((indices.shape[0],), self.unique_virus_ids[i], dtype=torch.long) \n                                      for i, indices in enumerate(self.virus_bag_indices_list)]\n\n      \n        self.datas = self.datas.cpu()\n\n\n    def __len__(self):\n        return len(self.unique_virus_ids)  # Number of unique viruses (outer bags)\n\n    def __getitem__(self, index):\n        \"\"\" Return outer bag (virus), inner bags (sequences), and instance-level data. \"\"\"\n        \n        # Get all instance indices belonging to this virus\n        virus_instance_indices = self.virus_bag_indices_list[index]\n        # Retrieve instance-level data\n        virus_data = self.datas[virus_instance_indices]\n        virus_label = self.virus_labels[index]\n        virus_id = self.precomputed_virus_ids[index]\n        # Find which sequences belong to this virus\n       \n        seq_ids_in_virus = self.virus_seq_map[virus_id[0].item()]\n\n        return {\n            \"virus_id\": virus_id,\n            \"virus_data\": virus_data,\n            \"virus_label\": virus_label,\n            \"seq_id\": seq_ids_in_virus\n        }\n\n\ndef collate_fn(batch):\n    \"\"\" Custom collate function for Bag-of-Bags MIL \"\"\"\n\n    batch_size = len(batch)\n\n    all_virus_ids = []\n    all_virus_data = []\n    all_virus_labels = []\n    all_virus_seq_ids = []\n   \n\n    for item in batch:\n        virus_id = item[\"virus_id\"].tolist()\n        virus_data = item[\"virus_data\"].tolist()\n        virus_label = item[\"virus_label\"]\n        seq_id = item[\"seq_id\"]\n\n        all_virus_seq_ids.extend(seq_id)\n        all_virus_ids.extend(virus_id)\n        all_virus_data.extend(virus_data)\n        all_virus_labels.append(virus_label)\n    \n    # Convert to tensors\n    batch_virus_labels = torch.tensor(all_virus_labels, dtype=torch.float)\n    batch_seq_ids = torch.tensor(all_virus_seq_ids, dtype=torch.float)\n    batch_virus_datas = torch.tensor(all_virus_data, dtype=torch.float)\n    batch_virus_ids = torch.tensor(all_virus_ids, dtype=torch.float)\n\n\n    return batch_virus_datas, batch_virus_ids,batch_seq_ids, batch_virus_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.651755Z","iopub.execute_input":"2025-03-29T12:30:28.652017Z","iopub.status.idle":"2025-03-29T12:30:28.670983Z","shell.execute_reply.started":"2025-03-29T12:30:28.651989Z","shell.execute_reply":"2025-03-29T12:30:28.670436Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train(epoch,dataloader):\n    model.train()\n    running_loss=0.\n    acc=0\n    human=0\n    animal=0\n    total_samples=0\n    \n\n\n    for batch_data, batch_ids,batch_seq_ids, batch_labels in tqdm(dataloader, desc=\"Processing Batches\"):\n        batch_data,batch_ids,batch_seq_ids, batch_labels  = batch_data.to(device), batch_ids.to(device),batch_seq_ids.to(device), batch_labels.to(device)\n        Y_prob, Y_hat, A =model(batch_data,batch_ids,batch_seq_ids)\n        Y_prob=Y_prob.squeeze(1)\n        loss = criterion(Y_prob, batch_labels)\n        # Optimizer step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        acc += ((Y_hat == batch_labels).sum().item())\n        total_samples += batch_labels.size(0)  # Track the total number of samples processed\n        human+=np.sum(Y_hat.cpu().numpy() == 0)\n        animal+=np.sum(Y_hat.cpu().numpy() == 1)\n    print(f'Epoch: {epoch}, Loss: {running_loss:.4f}, LR: {scheduler.get_last_lr()}')\n    acc=acc/total_samples*100\n    print(f'acc: {acc:.1f}%')\n    print(\"human = \",human)\n    print(\"animal = \",animal)\n    return running_loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.671626Z","iopub.execute_input":"2025-03-29T12:30:28.671869Z","iopub.status.idle":"2025-03-29T12:30:28.685171Z","shell.execute_reply.started":"2025-03-29T12:30:28.671850Z","shell.execute_reply":"2025-03-29T12:30:28.684566Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def test(dataloader):\n    # print(len(labels))\n    model.eval()\n    acc=0\n    human=0\n    animal=0\n    total_samples=0\n    output=[]\n    b_lables=[]\n    with torch.no_grad():\n         for batch_data, batch_ids,batch_seq_ids, batch_labels in tqdm(dataloader, desc=\"Processing Batches\"):\n            batch_data,batch_ids,batch_seq_ids, batch_labels  = batch_data.to(device), batch_ids.to(device),batch_seq_ids.to(device), batch_labels.to(device)\n            Y_prob, Y_hat, A =model(batch_data,batch_ids,batch_seq_ids)         \n            # output+=out_embed\n            # b_lables+=batch_labels\n            Y_prob=Y_prob.squeeze(1)\n            Y_hat = Y_hat.view_as(batch_labels)\n            acc += ((Y_hat == batch_labels).sum().item())\n            human+=np.sum(Y_hat.cpu().numpy() == 0)\n            animal+=np.sum(Y_hat.cpu().numpy() == 1)\n            total_samples += batch_labels.size(0)  # Track the total number of samples processed\n\n    acc=acc/total_samples*100\n    print(f'acc: {acc:.1f}%')\n    print(\"human = \",human)\n    print(\"animal = \",animal)\n\n    return output,b_lables","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.685993Z","iopub.execute_input":"2025-03-29T12:30:28.686278Z","iopub.status.idle":"2025-03-29T12:30:28.702515Z","shell.execute_reply.started":"2025-03-29T12:30:28.686250Z","shell.execute_reply":"2025-03-29T12:30:28.701839Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# read dataset\nfileNameOriginalDatas=\"/kaggle/input/ncbi-data-csv/ncbi_data.csv\"\ndf=readDataFromFile(fileNameOriginalDatas)\n\n# get the length of the longest seq\nllongest=max(df['Length'])\nlshortest=min(df['Length'])\nprint(\"llongest\",llongest)\nllongest=max(df['Length'])\nprint(\"lshortest\",lshortest)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:28.704930Z","iopub.execute_input":"2025-03-29T12:30:28.705149Z","iopub.status.idle":"2025-03-29T12:30:31.784878Z","shell.execute_reply.started":"2025-03-29T12:30:28.705120Z","shell.execute_reply":"2025-03-29T12:30:31.783712Z"}},"outputs":[{"name":"stdout","text":"llongest 775\nlshortest 202\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"n=193\nlower_bound = int(llongest / n)\nupper_bound = int(llongest - n + 1)\nl_sub_array=np.arange(lower_bound, upper_bound + 1)\nl_sub=lshortest-n+1\nif l_sub not in l_sub_array:\n    print(\"error ASW\")\nprint(l_sub)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:31.786804Z","iopub.execute_input":"2025-03-29T12:30:31.787123Z","iopub.status.idle":"2025-03-29T12:30:31.793145Z","shell.execute_reply.started":"2025-03-29T12:30:31.787089Z","shell.execute_reply":"2025-03-29T12:30:31.792134Z"}},"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\ndf[\"Class\"] = df[\"Class\"].str.lower()  #Ensure consistent casing\nlabels = np.array((df[\"Class\"] != \"human\").astype(int))\nids=df[\"Virus_ID\"]\nseq_ids=df[\"Seq_ID\"]+\" \"+df[\"Virus_ID\"]\n\n# convert string id to numeric\n_,ids = np.unique(ids, return_inverse=True)\n_,seq_ids = np.unique(seq_ids, return_inverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:31.794245Z","iopub.execute_input":"2025-03-29T12:30:31.794629Z","iopub.status.idle":"2025-03-29T12:30:32.200761Z","shell.execute_reply.started":"2025-03-29T12:30:31.794588Z","shell.execute_reply":"2025-03-29T12:30:32.199709Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"datas=df[\"Sequence\"]\n# Get unique bag IDs\nunique_bag_ids = np.unique(ids)\n\n# Split bag IDs into train and test\ntrain_ids, test_val_ids = train_test_split(unique_bag_ids, test_size=0.5, random_state=42)\ntest_ids, val_ids = train_test_split(test_val_ids, test_size=0.001, random_state=42)\n\n# Get indices corresponding to train/test bag IDs\ntrain_indices = np.where(np.isin(ids, train_ids))[0]\ntest_indices = np.where(np.isin(ids, test_ids))[0]\nval_indices = np.where(np.isin(ids, val_ids))[0]\n\n# # Create train data\ntrain_datas = datas[train_indices]\ntrain_ids = ids[train_indices]\ntrain_seq_ids = seq_ids[train_indices]\ntrain_labels = labels[train_indices]\n\n# # Create test data\ntest_datas = datas[test_indices]\ntest_ids = ids[test_indices]\ntest_seq_ids = seq_ids[test_indices]\ntest_labels = labels[test_indices]\n\n\n# # Create val data\nval_datas = datas[val_indices]\nval_ids = ids[val_indices]\nval_seq_ids = seq_ids[val_indices]\nval_labels = labels[val_indices]\n\nprint(train_datas.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:52:00.253774Z","iopub.execute_input":"2025-03-29T14:52:00.254069Z","iopub.status.idle":"2025-03-29T14:52:01.129251Z","shell.execute_reply.started":"2025-03-29T14:52:00.254045Z","shell.execute_reply":"2025-03-29T14:52:01.128421Z"}},"outputs":[{"name":"stdout","text":"(96487,)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# need validation\nsg_embed_size=30\nsg_window=5\n# Transformer Parameters\nnhead = 5         # Number of attention heads\nencoderNlayers = 2       # Number of transformer layers\nembeddingSize=sg_embed_size\nintermidiateDim=512\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:32.247060Z","iopub.execute_input":"2025-03-29T12:30:32.247450Z","iopub.status.idle":"2025-03-29T12:30:32.251648Z","shell.execute_reply.started":"2025-03-29T12:30:32.247415Z","shell.execute_reply":"2025-03-29T12:30:32.250788Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Apply ASW \n\ntrain_datas = [ASW(sequence,l_sub, n) for sequence in train_datas.tolist()]\n\ntrain_labels= np.repeat(train_labels, n).tolist()\ntrain_ids=np.repeat(train_ids, n).tolist()\ntrain_seq_ids=np.repeat(train_seq_ids, n).tolist()\n\nprint(len(train_datas))\nprint(len(train_datas[0]))\nprint(len(train_datas[0][0]))\n# Apply skip gram\n# Convert k-mers into embeddings\nw2v_model = Word2Vec(sentences=tqdm(train_datas, desc=\" Skip gram Training\"), vector_size=sg_embed_size, window=sg_window, sg=1, min_count=1, workers=5)\n# word_vectors = KeyedVectors.load(\"/kaggle/working/word2vec_vectors.kv\")\n\ntrain_seq_embeddings = np.array([w2v_model.wv[kmer] for kmer in tqdm(train_datas,desc=\"Skip gram inference\")])\ntrain_seq_embeddings = np.array(list(chain.from_iterable(train_seq_embeddings)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:30:32.252484Z","iopub.execute_input":"2025-03-29T12:30:32.252775Z","iopub.status.idle":"2025-03-29T12:34:54.172281Z","shell.execute_reply.started":"2025-03-29T12:30:32.252753Z","shell.execute_reply":"2025-03-29T12:34:54.171361Z"}},"outputs":[{"name":"stdout","text":"96487\n193\n10\n","output_type":"stream"},{"name":"stderr","text":" Skip gram Training: 100%|██████████| 96487/96487 [00:02<00:00, 34078.43it/s]\nSkip gram inference: 100%|██████████| 96487/96487 [00:33<00:00, 2868.66it/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train_seq_embeddings=torch.tensor(train_seq_embeddings).to(device)\ntrain_ids=torch.tensor(train_ids).to(device)\ntrain_seq_ids=torch.tensor(train_seq_ids).to(device)\ntrain_labels=torch.tensor(train_labels).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:34:54.173340Z","iopub.execute_input":"2025-03-29T12:34:54.173651Z","iopub.status.idle":"2025-03-29T12:35:05.406632Z","shell.execute_reply.started":"2025-03-29T12:34:54.173629Z","shell.execute_reply":"2025-03-29T12:35:05.405862Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_mildataset = MILDataset(train_seq_embeddings, train_ids,train_seq_ids, train_labels)\ntrain_loader = DataLoader(train_mildataset, batch_size=16, shuffle=True,num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:35:05.407333Z","iopub.execute_input":"2025-03-29T12:35:05.407571Z","iopub.status.idle":"2025-03-29T12:38:08.081545Z","shell.execute_reply.started":"2025-03-29T12:35:05.407552Z","shell.execute_reply":"2025-03-29T12:38:08.080701Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 41854/41854 [00:52<00:00, 799.26it/s]\n100%|██████████| 96487/96487 [02:01<00:00, 795.42it/s]\n41854it [00:04, 9279.44it/s] \n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Auto-detect GPU\nmodel = GatedAttention(nhead,encoderNlayers,embeddingSize,intermidiateDim).to(device)  # Move model to GPU\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\ncriterion = nn.BCELoss().to(device)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n\n\nprint(f\"Using device: {device}\") \nprint('Start Training')\nfor epoch in range(1, 10+1):\n    loss = train(epoch, train_loader)\n    scheduler.step(loss)  # Update LR based on loss\n    if scheduler.num_bad_epochs >= 5:  # Stop after 10 consecutive non-improving epochs\n        print(f\"Stopping early: No improvement for {scheduler.num_bad_epochs} epochs\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:38:08.082495Z","iopub.execute_input":"2025-03-29T12:38:08.082814Z","iopub.status.idle":"2025-03-29T14:13:39.189225Z","shell.execute_reply.started":"2025-03-29T12:38:08.082785Z","shell.execute_reply":"2025-03-29T14:13:39.188085Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nStart Training\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches:   0%|          | 0/2616 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n  return F.conv1d(\nProcessing Batches: 100%|██████████| 2616/2616 [09:35<00:00,  4.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss: 501.5304, LR: [0.001]\nacc: 830.0%\nhuman =  19810\nanimal =  22044\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:33<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss: 331.8105, LR: [0.001]\nacc: 842.5%\nhuman =  19946\nanimal =  21908\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:34<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Loss: 290.0883, LR: [0.001]\nacc: 845.3%\nhuman =  19938\nanimal =  21916\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:34<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Loss: 262.2073, LR: [0.001]\nacc: 845.8%\nhuman =  19925\nanimal =  21929\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:32<00:00,  4.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Loss: 253.3361, LR: [0.001]\nacc: 848.7%\nhuman =  19963\nanimal =  21891\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:32<00:00,  4.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Loss: 243.9743, LR: [0.001]\nacc: 848.9%\nhuman =  19964\nanimal =  21890\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:30<00:00,  4.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Loss: 236.1119, LR: [0.001]\nacc: 848.2%\nhuman =  19949\nanimal =  21905\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:33<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Loss: 230.4979, LR: [0.001]\nacc: 849.3%\nhuman =  19967\nanimal =  21887\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:31<00:00,  4.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Loss: 226.4064, LR: [0.001]\nacc: 849.6%\nhuman =  19957\nanimal =  21897\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [09:31<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Loss: 229.5003, LR: [0.001]\nacc: 849.2%\nhuman =  19938\nanimal =  21916\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"torch.save(model, \"model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:13:39.190902Z","iopub.execute_input":"2025-03-29T14:13:39.191877Z","iopub.status.idle":"2025-03-29T14:13:39.213827Z","shell.execute_reply.started":"2025-03-29T14:13:39.191851Z","shell.execute_reply":"2025-03-29T14:13:39.213180Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(np.sum(train_labels.cpu().numpy() == 0))\nprint(np.sum(train_labels.cpu().numpy() == 1))\n\n# print(np.sum(test_labels.cpu().numpy() == 0))\n# print(np.sum(test_labels.cpu().numpy() == 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:13:39.214592Z","iopub.execute_input":"2025-03-29T14:13:39.214810Z","iopub.status.idle":"2025-03-29T14:13:39.418952Z","shell.execute_reply.started":"2025-03-29T14:13:39.214790Z","shell.execute_reply":"2025-03-29T14:13:39.418237Z"}},"outputs":[{"name":"stdout","text":"6820620\n11801371\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print('Start Testing on train')\nout=test(train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:42:47.216775Z","iopub.execute_input":"2025-03-29T14:42:47.217112Z","iopub.status.idle":"2025-03-29T14:45:41.393093Z","shell.execute_reply.started":"2025-03-29T14:42:47.217086Z","shell.execute_reply":"2025-03-29T14:45:41.391242Z"}},"outputs":[{"name":"stdout","text":"Start Testing on train\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 2616/2616 [02:54<00:00, 15.02it/s]","output_type":"stream"},{"name":"stdout","text":"acc: 97.8%\nhuman =  20059\nanimal =  21795\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Apply ASW \n\ntest_datas = [ASW(sequence,l_sub, n) for sequence in test_datas.tolist()]\n\ntest_labels= np.repeat(test_labels, n).tolist()\ntest_ids=np.repeat(test_ids, n).tolist()\ntest_seq_ids=np.repeat(test_seq_ids, n).tolist()\n\n\n# Apply skip gram\nkeys_wv=set(list(w2v_model.wv.key_to_index.keys()))\n   \n# Convert k-mers into embeddings\ntest_seq_embeddings = np.array([\n    w2v_model.wv[k] if k in keys_wv else np.zeros(30)\n    for kmer in tqdm(test_datas, desc=\"Skip gram inference\")  \n    for k in kmer  # kmer should be defined first\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:06:04.198818Z","iopub.execute_input":"2025-03-29T15:06:04.199151Z","iopub.status.idle":"2025-03-29T15:06:04.229481Z","shell.execute_reply.started":"2025-03-29T15:06:04.199123Z","shell.execute_reply":"2025-03-29T15:06:04.228289Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Skip gram inference:   0%|          | 0/96394 [00:00<?, ?it/s]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-01bb91c856f0>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# ])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m test_seq_embeddings = np.array([\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkmer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Skip gram inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-01bb91c856f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m test_seq_embeddings = np.array([\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkmer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Skip gram inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkmer\u001b[0m  \u001b[0;31m# kmer should be defined first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Key 'LKGIAPLQLR' not present\""],"ename":"KeyError","evalue":"\"Key 'LKGIAPLQLR' not present\"","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"sety=set()\nfor kmer in tqdm(test_datas, desc=\"Skip gram inference\"):  \n    for k in kmer:  # kmer should be defined first\n        if k not in keys_wv:\n            sety.add(k)\n\n\nprint(len(sety))\nprint(len(np.unique(test_datas)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:09:59.487226Z","iopub.execute_input":"2025-03-29T15:09:59.487579Z","iopub.status.idle":"2025-03-29T15:10:10.262888Z","shell.execute_reply.started":"2025-03-29T15:09:59.487548Z","shell.execute_reply":"2025-03-29T15:10:10.261899Z"}},"outputs":[{"name":"stderr","text":"\nSkip gram inference:   0%|          | 0/96394 [00:00<?, ?it/s]\u001b[A\nSkip gram inference:   6%|▌         | 5871/96394 [00:00<00:01, 58702.78it/s]\u001b[A\nSkip gram inference:  12%|█▏        | 11742/96394 [00:00<00:01, 56536.90it/s]\u001b[A\nSkip gram inference:  18%|█▊        | 17402/96394 [00:00<00:01, 54490.78it/s]\u001b[A\nSkip gram inference:  24%|██▍       | 23286/96394 [00:00<00:01, 56145.76it/s]\u001b[A\nSkip gram inference:  30%|███       | 28937/96394 [00:00<00:01, 56270.09it/s]\u001b[A\nSkip gram inference:  36%|███▌      | 34572/96394 [00:00<00:01, 55609.49it/s]\u001b[A\nSkip gram inference:  42%|████▏     | 40518/96394 [00:00<00:00, 56843.28it/s]\u001b[A\nSkip gram inference:  48%|████▊     | 46658/96394 [00:00<00:00, 58274.15it/s]\u001b[A\nSkip gram inference:  55%|█████▍    | 52645/96394 [00:00<00:00, 58766.26it/s]\u001b[A\nSkip gram inference:  61%|██████    | 58825/96394 [00:01<00:00, 59694.62it/s]\u001b[A\nSkip gram inference:  68%|██████▊   | 65123/96394 [00:01<00:00, 60692.40it/s]\u001b[A\nSkip gram inference:  74%|███████▍  | 71445/96394 [00:01<00:00, 61456.36it/s]\u001b[A\nSkip gram inference:  80%|████████  | 77593/96394 [00:01<00:00, 61251.19it/s]\u001b[A\nSkip gram inference:  87%|████████▋ | 83720/96394 [00:01<00:00, 60367.93it/s]\u001b[A\nSkip gram inference: 100%|██████████| 96394/96394 [00:01<00:00, 59363.14it/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"115011\n291204\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"test_seq_embeddings=torch.tensor(test_seq_embeddings).to(device)\ntest_ids=torch.tensor(test_ids).to(device)\ntest_seq_ids=torch.tensor(test_seq_ids).to(device)\ntest_labels=torch.tensor(test_labels).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:54:17.860904Z","iopub.execute_input":"2025-03-29T14:54:17.861284Z","iopub.status.idle":"2025-03-29T14:54:30.642102Z","shell.execute_reply.started":"2025-03-29T14:54:17.861259Z","shell.execute_reply":"2025-03-29T14:54:30.641349Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"test_mildataset = MILDataset(test_seq_embeddings, test_ids,test_seq_ids, test_labels)\ntest_loader = DataLoader(test_mildataset, batch_size=64, shuffle=True,num_workers=0, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:54:44.552291Z","iopub.execute_input":"2025-03-29T14:54:44.552600Z","iopub.status.idle":"2025-03-29T14:57:50.338444Z","shell.execute_reply.started":"2025-03-29T14:54:44.552576Z","shell.execute_reply":"2025-03-29T14:57:50.337689Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 41812/41812 [00:53<00:00, 788.79it/s]\n100%|██████████| 96394/96394 [02:01<00:00, 791.83it/s]\n41812it [00:04, 10362.60it/s]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"print('Start Testing on test data')\nout2=test(test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:57:52.903884Z","iopub.execute_input":"2025-03-29T14:57:52.904184Z","iopub.status.idle":"2025-03-29T15:05:11.406560Z","shell.execute_reply.started":"2025-03-29T14:57:52.904159Z","shell.execute_reply":"2025-03-29T15:05:11.405492Z"}},"outputs":[{"name":"stdout","text":"Start Testing on test data\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 654/654 [07:18<00:00,  1.49it/s]","output_type":"stream"},{"name":"stdout","text":"acc: 97.6%\nhuman =  20184\nanimal =  21628\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}