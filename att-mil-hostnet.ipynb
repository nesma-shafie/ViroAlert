{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":11465574,"datasetId":6981441,"databundleVersionId":11908442}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install --no-cache-dir gensim\n# import gensim\n# print(gensim.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.454073Z","iopub.execute_input":"2025-04-18T19:44:36.454466Z","iopub.status.idle":"2025-04-18T19:44:36.458556Z","shell.execute_reply.started":"2025-04-18T19:44:36.454406Z","shell.execute_reply":"2025-04-18T19:44:36.457514Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom __future__ import print_function\nimport argparse\nimport numpy as np\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\nfrom collections import Counter\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec,KeyedVectors\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nimport os\nimport pandas as pd\nfrom itertools import chain\ntorch.manual_seed(42)\nnp.random.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Auto-detect GPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.459664Z","iopub.execute_input":"2025-04-18T19:44:36.459860Z","iopub.status.idle":"2025-04-18T19:44:36.475175Z","shell.execute_reply.started":"2025-04-18T19:44:36.459842Z","shell.execute_reply":"2025-04-18T19:44:36.474455Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def readDataFromFile(filename):\n    file_path = os.path.abspath(filename)  # Ensure absolute path\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n\n    # Rename columns for clarity\n    df.columns = [\"ID\", \"Sequence\"]\n\n    # Extract virus_ID (part after first \"|\") and seq_ID (part after second \"|\")\n    df[\"Virus_ID\"] = df[\"ID\"].apply(lambda x: \"\".join(x.split(\"|\")[1:]) if \"|\" in x else \"\")\n    df[\"Seq_ID\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[0] if \"|\" in x else \"\")\n    df[\"Class\"] = df[\"ID\"].apply(lambda x: x.split(\"|\")[-1] if \"|\" in x else \"\")\n    df[\"Length\"] = df[\"Sequence\"].apply(lambda x: len(x))\n\n    return df[[\"Sequence\",\"Virus_ID\", \"Seq_ID\", \"Class\",\"Length\"]]  # Return relevant columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.476840Z","iopub.execute_input":"2025-04-18T19:44:36.477066Z","iopub.status.idle":"2025-04-18T19:44:36.489598Z","shell.execute_reply.started":"2025-04-18T19:44:36.477046Z","shell.execute_reply":"2025-04-18T19:44:36.488964Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def ASW(sequence, l_sub, n):\n    \"\"\"\n        sequence (str): The original viral sequence.\n        l_sub (int): The length of each subsequence.\n        n (int): The number of subsequences to generate.\n    \"\"\"\n    l = len(sequence)\n    \n \n    if n > 1:\n        l_stride = (l - l_sub) // (n - 1)\n    else:\n        l_stride = 1  \n    \n    subsequences = []\n \n    for i in range(0, min(n * l_stride, l - l_sub + 1), l_stride):\n        subsequences.append(sequence[i:i + l_sub])\n    \n    return subsequences\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.490560Z","iopub.execute_input":"2025-04-18T19:44:36.490845Z","iopub.status.idle":"2025-04-18T19:44:36.511931Z","shell.execute_reply.started":"2025-04-18T19:44:36.490816Z","shell.execute_reply":"2025-04-18T19:44:36.511069Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# 0->human  1-> animals\nclass GatedAttention(nn.Module):\n    def __init__(self,nhead,encoderNlayers,embeddingSize,intermidiateDim):\n        super(GatedAttention, self).__init__()\n        self.M = embeddingSize\n        self.L = intermidiateDim\n        self.encoderNlayers=encoderNlayers\n        self.ATTENTION_BRANCHES = 1\n        self.nhead=nhead\n\n        # embedding \n        self.encoder_layer = TransformerEncoderLayer(d_model=self.M, nhead=self.nhead)\n        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=self.encoderNlayers)\n        \n        # instance level \n        self.attention_V_1 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix V\n            nn.Tanh()\n        )\n\n        self.attention_U_1 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix U\n            nn.Sigmoid()\n        )\n\n        self.attention_w_1 = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n\n\n        # bag level \n        self.attention_V_2 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix V\n            nn.Tanh()\n        )\n\n        self.attention_U_2 = nn.Sequential(\n            nn.Linear(self.M, self.L), # matrix U\n            nn.Sigmoid()\n        )\n\n        self.attention_w_2 = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n\n\n        \n        # classifier\n        self.classifier = nn.Sequential(\n            nn.Conv1d(in_channels=self.ATTENTION_BRANCHES, out_channels=128, kernel_size=4, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2),\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=7, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2),\n            nn.Flatten(),  # Converts to 1D before fully connected layers\n            nn.Linear(128 * ((self.M) // 4), 256),  # Adjust size based on sequence length\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 1),\n            nn.Sigmoid() \n        )\n\n    \n    def forward(self, datas,ids,Seq_ids):\n        #### STEP 1:embeddings\n        datas = datas.float()  # Ensure correct dtype\n        instances=self.transformer_encoder(datas) \n        \n        #### STEP 2: INSTANCE-LEVEL ATTENTION ####\n        # Apply attention mechanisms per bag (over instances_per_bag)\n        A_V = self.attention_V_1(instances)  \n        A_U = self.attention_U_1(instances)  \n        A = self.attention_w_1(A_V * A_U)\n        A = torch.transpose(A, 1, 0)  \n        inner_bags = torch.unique_consecutive(Seq_ids)\n      \n        output = torch.empty(((len(inner_bags), self.M))).to(device)\n        super_ids = torch.empty(((len(inner_bags))))\n        for i, bag in enumerate(inner_bags):\n            A_vec=F.softmax(A[0][Seq_ids == bag],dim=0)\n            output[i] = torch.matmul(A_vec, instances[Seq_ids == bag])\n            super_ids[i]=ids[Seq_ids == bag][0]\n        \n        ### STEP 3: BAG-LEVEL ATTENTION ####\n        A_V_2 = self.attention_V_2(output)  \n        A_U_2 = self.attention_U_2(output)  \n        A_2 = self.attention_w_2(A_V_2 * A_U_2)  \n        A_2 = torch.transpose(A_2, 1,0)   \n\n      \n        outer_bags = torch.unique_consecutive(super_ids)\n        output2 = torch.empty(((len(outer_bags), self.M))).to(device)\n\n        for i, bag in enumerate(outer_bags):\n            A_vec_2=F.softmax(A_2[0][super_ids == bag],dim=0)\n            output2[i] = torch.matmul(A_vec_2, output[super_ids == bag])\n\n        \n        \n        ### STEP 4: CLASSIFICATION ####\n        # output2 = output2.view(output2.shape[0], -1)  # Flatten over bags_per_bag for classification\n        output2 = output2.unsqueeze(1)  # Add a channel dimension\n\n\n        Y_prob = self.classifier(output2)  # Shape: [batch_size, 1]\n        Y_hat = torch.ge(Y_prob, 0.5).float()  # Convert probabilities to binary predictions\n        return Y_prob, Y_hat, A","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.512895Z","iopub.execute_input":"2025-04-18T19:44:36.513200Z","iopub.status.idle":"2025-04-18T19:44:36.528128Z","shell.execute_reply.started":"2025-04-18T19:44:36.513168Z","shell.execute_reply":"2025-04-18T19:44:36.527346Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class MILDataset(Dataset):\n    def __init__(self, datas, ids, seq_ids, labels):\n        self.datas = datas  # Instance features\n        self.ids = ids # Virus (outer bag) IDs\n        self.seq_ids = seq_ids  # Sequence (inner bag) IDs\n        self.labels = labels.to(\"cpu\")  # Labels at the virus (outer bag) level\n\n        # Unique IDs for outer bags (viruses) and their indices\n        self.unique_virus_ids, self.virus_indices = torch.unique(self.ids, return_inverse=True)\n        \n        # Unique IDs for inner bags (sequences) and their indices\n        self.unique_seq_ids, self.seq_indices = torch.unique(self.seq_ids, return_inverse=True)\n\n        # Mapping from virus to instance indices  2d array each list is the virus data indecies\n        self.virus_bag_indices_list = [torch.where(self.virus_indices == i)[0].to(\"cpu\") for i in tqdm(range(len(self.unique_virus_ids)))]\n\n        # Mapping from sequence to instance indices 2d array each list is the seq data indecies\n        self.seq_bag_indices_list = [torch.where(self.seq_indices == i)[0].to(\"cpu\") for i in tqdm( range(len(self.unique_seq_ids)))]\n\n        # Labels assigned at the virus level (each virus gets one label)\n        self.virus_labels = [self.labels[indices[0]] for indices in self.virus_bag_indices_list]\n\n        # Precomputed bag-of-bags structure (virus → [seq])\n        self.virus_seq_map = {}  # Maps virus_id -> list of sequence indices\n        for i, virus_id in tqdm(enumerate(self.unique_virus_ids)):\n            self.virus_seq_map[virus_id.item()] = list((self.seq_ids[self.virus_bag_indices_list[i]].tolist()))\n\n        # Precomputed bag IDs for each virus and sequence\n        self.precomputed_virus_ids = [torch.full((indices.shape[0],), self.unique_virus_ids[i], dtype=torch.long) \n                                      for i, indices in enumerate(self.virus_bag_indices_list)]\n\n      \n        self.datas = self.datas.cpu()\n\n\n    def __len__(self):\n        return len(self.unique_virus_ids)  # Number of unique viruses (outer bags)\n\n    def __getitem__(self, index):\n        \"\"\" Return outer bag (virus), inner bags (sequences), and instance-level data. \"\"\"\n        \n        # Get all instance indices belonging to this virus\n        virus_instance_indices = self.virus_bag_indices_list[index]\n        # Retrieve instance-level data\n        virus_data = self.datas[virus_instance_indices]\n        virus_label = self.virus_labels[index]\n        virus_id = self.precomputed_virus_ids[index]\n        # Find which sequences belong to this virus\n       \n        seq_ids_in_virus = self.virus_seq_map[virus_id[0].item()]\n\n        return {\n            \"virus_id\": virus_id,\n            \"virus_data\": virus_data,\n            \"virus_label\": virus_label,\n            \"seq_id\": seq_ids_in_virus\n        }\n\n\ndef collate_fn(batch):\n    \"\"\" Custom collate function for Bag-of-Bags MIL \"\"\"\n\n    batch_size = len(batch)\n\n    all_virus_ids = []\n    all_virus_data = []\n    all_virus_labels = []\n    all_virus_seq_ids = []\n   \n\n    for item in batch:\n        virus_id = item[\"virus_id\"].tolist()\n        virus_data = item[\"virus_data\"].tolist()\n        virus_label = item[\"virus_label\"]\n        seq_id = item[\"seq_id\"]\n\n        all_virus_seq_ids.extend(seq_id)\n        all_virus_ids.extend(virus_id)\n        all_virus_data.extend(virus_data)\n        all_virus_labels.append(virus_label)\n    \n    # Convert to tensors\n    batch_virus_labels = torch.tensor(all_virus_labels, dtype=torch.float)\n    batch_seq_ids = torch.tensor(all_virus_seq_ids, dtype=torch.float)\n    batch_virus_datas = torch.tensor(all_virus_data, dtype=torch.float)\n    batch_virus_ids = torch.tensor(all_virus_ids, dtype=torch.float)\n\n\n    return batch_virus_datas, batch_virus_ids,batch_seq_ids, batch_virus_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.557136Z","iopub.execute_input":"2025-04-18T19:44:36.557340Z","iopub.status.idle":"2025-04-18T19:44:36.568937Z","shell.execute_reply.started":"2025-04-18T19:44:36.557323Z","shell.execute_reply":"2025-04-18T19:44:36.567847Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def train(epoch,dataloader):\n    model.train()\n    running_loss=0.\n    acc=0\n    human=0\n    animal=0\n    total_samples=0\n    \n    for batch_data, batch_ids,batch_seq_ids, batch_labels in tqdm(dataloader, desc=\"Processing Batches\"):\n        batch_data,batch_ids,batch_seq_ids, batch_labels  = batch_data.to(device), batch_ids.to(device),batch_seq_ids.to(device), batch_labels.to(device)\n        Y_prob, Y_hat, A =model(batch_data,batch_ids,batch_seq_ids)\n        Y_prob=Y_prob.squeeze(1)\n        Y_hat = Y_hat.view_as(batch_labels)\n        loss = criterion(Y_prob, batch_labels)\n        # Optimizer step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        acc += ((Y_hat == batch_labels).sum().item())\n        total_samples += batch_labels.size(0)  # Track the total number of samples processed\n        human+=np.sum(Y_hat.cpu().numpy() == 0)\n        animal+=np.sum(Y_hat.cpu().numpy() == 1)\n    print(f'Epoch: {epoch}, Loss: {running_loss:.4f}, LR: {scheduler.get_last_lr()}')\n    acc=acc/total_samples*100\n    print(f'acc: {acc:.1f}%')\n    print(\"human = \",human)\n    print(\"animal = \",animal)\n    return running_loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.570268Z","iopub.execute_input":"2025-04-18T19:44:36.570587Z","iopub.status.idle":"2025-04-18T19:44:36.592098Z","shell.execute_reply.started":"2025-04-18T19:44:36.570558Z","shell.execute_reply":"2025-04-18T19:44:36.591319Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def test(dataloader):\n    # print(len(labels))\n    model.eval()\n    acc=0\n    human=0\n    animal=0\n    total_samples=0\n    output=[]\n    b_lables=[]\n    with torch.no_grad():\n         for batch_data, batch_ids,batch_seq_ids, batch_labels in tqdm(dataloader, desc=\"Processing Batches\"):\n            batch_data,batch_ids,batch_seq_ids, batch_labels  = batch_data.to(device), batch_ids.to(device),batch_seq_ids.to(device), batch_labels.to(device)\n            Y_prob, Y_hat, A =model(batch_data,batch_ids,batch_seq_ids)         \n            # output+=out_embed\n            # b_lables+=batch_labels\n            Y_prob=Y_prob.squeeze(1)\n            Y_hat = Y_hat.view_as(batch_labels)\n            acc += ((Y_hat == batch_labels).sum().item())\n            human+=np.sum(Y_hat.cpu().numpy() == 0)\n            animal+=np.sum(Y_hat.cpu().numpy() == 1)\n            total_samples += batch_labels.size(0)  # Track the total number of samples processed\n\n    acc=acc/total_samples*100\n    print(f'acc: {acc:.1f}%')\n    print(\"human = \",human)\n    print(\"animal = \",animal)\n\n    return output,b_lables","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.593256Z","iopub.execute_input":"2025-04-18T19:44:36.593494Z","iopub.status.idle":"2025-04-18T19:44:36.608627Z","shell.execute_reply.started":"2025-04-18T19:44:36.593473Z","shell.execute_reply":"2025-04-18T19:44:36.607996Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# read dataset\nfileNameOriginalDatas=\"/kaggle/input/ncbi-data-csv/ncbi_data.csv\"\ndf=readDataFromFile(fileNameOriginalDatas)\n\n# get the length of the longest seq\nllongest=max(df['Length'])\nlshortest=min(df['Length'])\nprint(\"llongest\",llongest)\nllongest=max(df['Length'])\nprint(\"lshortest\",lshortest)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:36.609570Z","iopub.execute_input":"2025-04-18T19:44:36.609777Z","iopub.status.idle":"2025-04-18T19:44:37.100935Z","shell.execute_reply.started":"2025-04-18T19:44:36.609758Z","shell.execute_reply":"2025-04-18T19:44:37.099982Z"}},"outputs":[{"name":"stdout","text":"llongest 775\nlshortest 202\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"n=193\nlower_bound = int(llongest / n)\nupper_bound = int(llongest - n + 1)\nl_sub_array=np.arange(lower_bound, upper_bound + 1)\nl_sub=lshortest-n+1\nif l_sub not in l_sub_array:\n    print(\"error ASW\")\nprint(l_sub)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:37.101869Z","iopub.execute_input":"2025-04-18T19:44:37.102200Z","iopub.status.idle":"2025-04-18T19:44:37.108959Z","shell.execute_reply.started":"2025-04-18T19:44:37.102165Z","shell.execute_reply":"2025-04-18T19:44:37.108112Z"}},"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"\ndf[\"Class\"] = df[\"Class\"].str.lower()  #Ensure consistent casing\nlabels = np.array((df[\"Class\"] != \"human\").astype(int))\nids=df[\"Virus_ID\"]\nseq_ids=df[\"Seq_ID\"]+\" \"+df[\"Virus_ID\"]\n\n# convert string id to numeric\n_,ids = np.unique(ids, return_inverse=True)\n_,seq_ids = np.unique(seq_ids, return_inverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:37.109890Z","iopub.execute_input":"2025-04-18T19:44:37.110234Z","iopub.status.idle":"2025-04-18T19:44:37.207598Z","shell.execute_reply.started":"2025-04-18T19:44:37.110202Z","shell.execute_reply":"2025-04-18T19:44:37.206911Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"datas=df[\"Sequence\"]\n# Get unique bag IDs\nunique_bag_ids = np.unique(ids)\n\n# Split bag IDs into train and test\ntrain_ids, test_val_ids = train_test_split(unique_bag_ids, test_size=0.5, random_state=42)\ntest_ids, val_ids = train_test_split(test_val_ids, test_size=0.001, random_state=42)\n\n# Get indices corresponding to train/test bag IDs\ntrain_indices = np.where(np.isin(ids, train_ids))[0]\ntest_indices = np.where(np.isin(ids, test_ids))[0]\nval_indices = np.where(np.isin(ids, val_ids))[0]\n\n# # Create train data\ntrain_datas = datas[train_indices]\ntrain_ids = ids[train_indices]\ntrain_seq_ids = seq_ids[train_indices]\ntrain_labels = labels[train_indices]\n\n# # Create test data\ntest_datas = datas[test_indices]\ntest_ids = ids[test_indices]\ntest_seq_ids = seq_ids[test_indices]\ntest_labels = labels[test_indices]\n\n\n# # Create val data\nval_datas = datas[val_indices]\nval_ids = ids[val_indices]\nval_seq_ids = seq_ids[val_indices]\nval_labels = labels[val_indices]\n\nprint(train_datas.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:37.208352Z","iopub.execute_input":"2025-04-18T19:44:37.208627Z","iopub.status.idle":"2025-04-18T19:44:37.440783Z","shell.execute_reply.started":"2025-04-18T19:44:37.208605Z","shell.execute_reply":"2025-04-18T19:44:37.439833Z"}},"outputs":[{"name":"stdout","text":"(23974,)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# need validation\nsg_embed_size=30\nsg_window=5\n# Transformer Parameters\nnhead = 5         # Number of attention heads\nencoderNlayers = 2       # Number of transformer layers\nembeddingSize=sg_embed_size\nintermidiateDim=512\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:37.443532Z","iopub.execute_input":"2025-04-18T19:44:37.443792Z","iopub.status.idle":"2025-04-18T19:44:37.447968Z","shell.execute_reply.started":"2025-04-18T19:44:37.443769Z","shell.execute_reply":"2025-04-18T19:44:37.447038Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Apply ASW \n\ntrain_datas = [ASW(sequence,l_sub, n) for sequence in train_datas.tolist()]\n\ntrain_labels= np.repeat(train_labels, n).tolist()\ntrain_ids=np.repeat(train_ids, n).tolist()\ntrain_seq_ids=np.repeat(train_seq_ids, n).tolist()\n\nprint(len(train_datas))\nprint(len(train_datas[0]))\nprint(len(train_datas[0][0]))\n# Apply skip gram\n# Convert k-mers into embeddings\nw2v_model = Word2Vec(sentences=tqdm(train_datas, desc=\" Skip gram Training\"), vector_size=sg_embed_size, window=sg_window, sg=1, min_count=1, workers=5)\n# word_vectors = KeyedVectors.load(\"/kaggle/working/word2vec_vectors.kv\")\n\ntrain_seq_embeddings = np.array([w2v_model.wv[kmer] for kmer in tqdm(train_datas,desc=\"Skip gram inference\")])\ntrain_seq_embeddings = np.array(list(chain.from_iterable(train_seq_embeddings)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:44:37.449669Z","iopub.execute_input":"2025-04-18T19:44:37.450010Z","iopub.status.idle":"2025-04-18T19:45:49.165978Z","shell.execute_reply.started":"2025-04-18T19:44:37.449965Z","shell.execute_reply":"2025-04-18T19:45:49.165247Z"}},"outputs":[{"name":"stdout","text":"23974\n193\n10\n","output_type":"stream"},{"name":"stderr","text":" Skip gram Training: 100%|██████████| 23974/23974 [00:00<00:00, 33830.15it/s]\nSkip gram inference: 100%|██████████| 23974/23974 [00:08<00:00, 2811.85it/s]\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"train_seq_embeddings=torch.tensor(train_seq_embeddings).to(device)\ntrain_ids=torch.tensor(train_ids).to(device)\ntrain_seq_ids=torch.tensor(train_seq_ids).to(device)\ntrain_labels=torch.tensor(train_labels).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:45:49.168572Z","iopub.execute_input":"2025-04-18T19:45:49.168811Z","iopub.status.idle":"2025-04-18T19:45:51.733055Z","shell.execute_reply.started":"2025-04-18T19:45:49.168791Z","shell.execute_reply":"2025-04-18T19:45:51.732319Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"train_mildataset = MILDataset(train_seq_embeddings, train_ids,train_seq_ids, train_labels)\ntrain_loader = DataLoader(train_mildataset, batch_size=16, shuffle=True,num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:45:51.733826Z","iopub.execute_input":"2025-04-18T19:45:51.734051Z","iopub.status.idle":"2025-04-18T19:46:06.543668Z","shell.execute_reply.started":"2025-04-18T19:45:51.734032Z","shell.execute_reply":"2025-04-18T19:46:06.542703Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 14288/14288 [00:04<00:00, 3105.63it/s]\n100%|██████████| 23974/23974 [00:07<00:00, 3150.58it/s]\n14288it [00:01, 11820.22it/s]\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Auto-detect GPU\nmodel = GatedAttention(nhead,encoderNlayers,embeddingSize,intermidiateDim).to(device)  # Move model to GPU\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\ncriterion = nn.BCELoss().to(device)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n\n\nprint(f\"Using device: {device}\") \nprint('Start Training')\nfor epoch in range(1, 10+1):\n    loss = train(epoch, train_loader)\n    scheduler.step(loss)  # Update LR based on loss\n    if scheduler.num_bad_epochs >= 5:  # Stop after 10 consecutive non-improving epochs\n        print(f\"Stopping early: No improvement for {scheduler.num_bad_epochs} epochs\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:46:06.544648Z","iopub.execute_input":"2025-04-18T19:46:06.544906Z","iopub.status.idle":"2025-04-18T20:05:45.796607Z","shell.execute_reply.started":"2025-04-18T19:46:06.544883Z","shell.execute_reply":"2025-04-18T20:05:45.795466Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nStart Training\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:57<00:00,  7.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss: 286.7632, LR: [0.001]\nacc: 86.8%\nhuman =  6722\nanimal =  7566\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:57<00:00,  7.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss: 205.5633, LR: [0.001]\nacc: 91.7%\nhuman =  6903\nanimal =  7385\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:57<00:00,  7.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Loss: 178.0193, LR: [0.001]\nacc: 93.5%\nhuman =  7007\nanimal =  7281\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:57<00:00,  7.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Loss: 169.0328, LR: [0.001]\nacc: 94.0%\nhuman =  6989\nanimal =  7299\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:56<00:00,  7.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Loss: 159.4408, LR: [0.001]\nacc: 94.3%\nhuman =  6974\nanimal =  7314\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:58<00:00,  7.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Loss: 146.1880, LR: [0.001]\nacc: 95.1%\nhuman =  7038\nanimal =  7250\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:58<00:00,  7.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Loss: 139.3122, LR: [0.001]\nacc: 95.2%\nhuman =  7022\nanimal =  7266\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:58<00:00,  7.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Loss: 156.0882, LR: [0.001]\nacc: 94.4%\nhuman =  7025\nanimal =  7263\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:58<00:00,  7.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Loss: 139.6715, LR: [0.001]\nacc: 95.2%\nhuman =  7033\nanimal =  7255\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [01:58<00:00,  7.55it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Loss: 132.2841, LR: [0.001]\nacc: 95.5%\nhuman =  7035\nanimal =  7253\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"torch.save(model, \"model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:05:45.798241Z","iopub.execute_input":"2025-04-18T20:05:45.798506Z","iopub.status.idle":"2025-04-18T20:05:45.821514Z","shell.execute_reply.started":"2025-04-18T20:05:45.798481Z","shell.execute_reply":"2025-04-18T20:05:45.820741Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"print(np.sum(train_labels.cpu().numpy() == 0))\nprint(np.sum(train_labels.cpu().numpy() == 1))\n\n# print(np.sum(test_labels.cpu().numpy() == 0))\n# print(np.sum(test_labels.cpu().numpy() == 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:05:45.822731Z","iopub.execute_input":"2025-04-18T20:05:45.823042Z","iopub.status.idle":"2025-04-18T20:05:45.873233Z","shell.execute_reply.started":"2025-04-18T20:05:45.823010Z","shell.execute_reply":"2025-04-18T20:05:45.872577Z"}},"outputs":[{"name":"stdout","text":"1749159\n2877823\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"print('Start Testing on train')\nout=test(train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:05:45.874007Z","iopub.execute_input":"2025-04-18T20:05:45.874282Z","iopub.status.idle":"2025-04-18T20:06:18.637593Z","shell.execute_reply.started":"2025-04-18T20:05:45.874247Z","shell.execute_reply":"2025-04-18T20:06:18.636388Z"}},"outputs":[{"name":"stdout","text":"Start Testing on train\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 893/893 [00:32<00:00, 27.26it/s]","output_type":"stream"},{"name":"stdout","text":"acc: 95.7%\nhuman =  6925\nanimal =  7363\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Apply ASW \n\ntest_datas = [ASW(sequence,l_sub, n) for sequence in test_datas.tolist()]\n\ntest_labels= np.repeat(test_labels, n).tolist()\ntest_ids=np.repeat(test_ids, n).tolist()\ntest_seq_ids=np.repeat(test_seq_ids, n).tolist()\n\n\n# Apply skip gram\nkeys_wv=set(list(w2v_model.wv.key_to_index.keys()))\n   \n# Convert k-mers into embeddings\ntest_seq_embeddings = np.array([\n    w2v_model.wv[k] if k in keys_wv else np.zeros(30)\n    for kmer in tqdm(test_datas, desc=\"Skip gram inference\")  \n    for k in kmer  # kmer should be defined first\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:06:18.638629Z","iopub.execute_input":"2025-04-18T20:06:18.638884Z","iopub.status.idle":"2025-04-18T20:06:28.027087Z","shell.execute_reply.started":"2025-04-18T20:06:18.638861Z","shell.execute_reply":"2025-04-18T20:06:28.026092Z"}},"outputs":[{"name":"stderr","text":"Skip gram inference: 100%|██████████| 23828/23828 [00:05<00:00, 4373.97it/s]\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"sety=set()\nfor kmer in tqdm(test_datas, desc=\"Skip gram inference\"):  \n    for k in kmer:  # kmer should be defined first\n        if k not in keys_wv:\n            sety.add(k)\n\n\nprint(len(sety))\nprint(len(np.unique(test_datas)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:06:28.053584Z","iopub.execute_input":"2025-04-18T20:06:28.053857Z","iopub.status.idle":"2025-04-18T20:06:30.702930Z","shell.execute_reply.started":"2025-04-18T20:06:28.053833Z","shell.execute_reply":"2025-04-18T20:06:30.702042Z"}},"outputs":[{"name":"stderr","text":"Skip gram inference: 100%|██████████| 23828/23828 [00:00<00:00, 53195.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"75139\n172891\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"test_seq_embeddings=torch.tensor(test_seq_embeddings).to(device)\ntest_ids=torch.tensor(test_ids).to(device)\ntest_seq_ids=torch.tensor(test_seq_ids).to(device)\ntest_labels=torch.tensor(test_labels).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:06:30.703914Z","iopub.execute_input":"2025-04-18T20:06:30.704268Z","iopub.status.idle":"2025-04-18T20:06:33.609362Z","shell.execute_reply.started":"2025-04-18T20:06:30.704232Z","shell.execute_reply":"2025-04-18T20:06:33.608685Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"test_mildataset = MILDataset(test_seq_embeddings, test_ids,test_seq_ids, test_labels)\ntest_loader = DataLoader(test_mildataset, batch_size=64, shuffle=True,num_workers=0, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:06:33.610190Z","iopub.execute_input":"2025-04-18T20:06:33.610514Z","iopub.status.idle":"2025-04-18T20:06:48.292457Z","shell.execute_reply.started":"2025-04-18T20:06:33.610479Z","shell.execute_reply":"2025-04-18T20:06:48.291766Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 14273/14273 [00:04<00:00, 3122.59it/s]\n100%|██████████| 23828/23828 [00:07<00:00, 3170.84it/s]\n14273it [00:01, 12237.48it/s]\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"print('Start Testing on test data')\nout2=test(test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:06:48.293127Z","iopub.execute_input":"2025-04-18T20:06:48.293333Z","iopub.status.idle":"2025-04-18T20:08:13.889671Z","shell.execute_reply.started":"2025-04-18T20:06:48.293314Z","shell.execute_reply":"2025-04-18T20:08:13.888322Z"}},"outputs":[{"name":"stdout","text":"Start Testing on test data\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|██████████| 224/224 [01:25<00:00,  2.62it/s]","output_type":"stream"},{"name":"stdout","text":"acc: 95.1%\nhuman =  6946\nanimal =  7327\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}